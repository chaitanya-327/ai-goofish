import asyncio
import sys
import os
import argparse
import math
import json
import random
import base64
import re
import time
from datetime import datetime
from functools import wraps
from urllib.parse import urlencode
from typing import Optional, Dict, Any
import logging
from logging.handlers import RotatingFileHandler
from typing import Optional

import requests
from dotenv import load_dotenv
from openai import AsyncOpenAI, APIStatusError
from playwright.async_api import async_playwright, Response, TimeoutError as PlaywrightTimeoutError
from requests.exceptions import HTTPError

# æ·»åŠ æ•°æ®åº“å¯¼å…¥
from database import XianyuDatabase

# æ·»åŠ é‚®ä»¶æ¨¡å—å¯¼å…¥
from email_sender import email_sender

# æ·»åŠ Cookieç®¡ç†å™¨å¯¼å…¥
from cookie_manager import CookieManager

# æ·»åŠ ä»£ç†ç®¡ç†å™¨å¯¼å…¥
from proxy_manager import ProxyManager

# æ·»åŠ é€Ÿç‡é™åˆ¶å™¨å¯¼å…¥
from rate_limiter import RateLimiter, adaptive_sleep

"""
é—²é±¼å•†å“çˆ¬è™«ä¸»æ¨¡å— (Version 2)

å®ç°åŠŸèƒ½å®Œæ•´çš„é—²é±¼å•†å“çˆ¬è™«ç³»ç»Ÿï¼Œæ”¯æŒï¼š
- å¤šä»»åŠ¡å¹¶å‘çˆ¬å–
- Cookieæ± ç®¡ç†å’Œè½®æ¢
- ä»£ç†æ± ç®¡ç†å’Œè‡ªåŠ¨åˆ‡æ¢
- AIæ™ºèƒ½åˆ†æå’Œè¿‡æ»¤
- å®æ—¶é€šçŸ¥æ¨é€
- æ•°æ®åº“å­˜å‚¨å’Œç®¡ç†
- é‚®ä»¶é€šçŸ¥åŠŸèƒ½

ä¸»è¦ç»„ä»¶ï¼š
- å„ç§çˆ¬å–å’Œè§£æå‡½æ•°
- AIåˆ†æå’Œé€šçŸ¥åŠŸèƒ½

ä½œè€…ï¼šddCat
ç‰ˆæœ¬ï¼š1.0
åˆ›å»ºæ—¶é—´ï¼š2025-08-04
"""

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs('logs', exist_ok=True)
# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        RotatingFileHandler('logs/spider.log', maxBytes=10*1024*1024, backupCount=5, encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è¯»å–å•†å“è·³è¿‡é…ç½®ï¼Œé»˜è®¤ä¸ºtrue
SKIP_EXISTING_PRODUCTS = os.getenv('SKIP_EXISTING_PRODUCTS', 'true').lower() == 'true'

# å®šä¹‰ç™»å½•çŠ¶æ€æ–‡ä»¶çš„è·¯å¾„
STATE_FILE = "xianyu_state.json"
# å®šä¹‰é—²é±¼æœç´¢APIçš„URLç‰¹å¾
API_URL_PATTERN = "h5api.m.goofish.com/h5/mtop.taobao.idlemtopsearch.pc.search"
# å®šä¹‰é—²é±¼è¯¦æƒ…é¡µAPIçš„URLç‰¹å¾
DETAIL_API_URL_PATTERN = "h5api.m.goofish.com/h5/mtop.taobao.idle.pc.detail"

# --- AI & Notification Configuration ---
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
BASE_URL = os.getenv("OPENAI_BASE_URL")
MODEL_NAME = os.getenv("OPENAI_MODEL_NAME")
NTFY_TOPIC_URL = os.getenv("NTFY_TOPIC_URL")

# ä»£ç†é…ç½®
PROXY_API_URL = os.getenv('PROXY_API_URL')
PROXY_ENABLED = os.getenv('PROXY_ENABLED', 'false').lower() == 'true'
PROXY_RETRY_COUNT = int(os.getenv('PROXY_RETRY_COUNT', '3'))
PROXY_REFRESH_INTERVAL = int(os.getenv('PROXY_REFRESH_INTERVAL', '1800'))  # é»˜è®¤30åˆ†é’Ÿæ›´æ¢ä¸€æ¬¡ä»£ç†

# ç°ä»£æµè§ˆå™¨User-Agentæ± 
USER_AGENTS = [
    # Chrome æœ€æ–°ç‰ˆæœ¬
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",

    # Firefox æœ€æ–°ç‰ˆæœ¬
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0",

    # Edge æœ€æ–°ç‰ˆæœ¬
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
]

def get_random_user_agent() -> str:
    """
    è·å–éšæœºçš„ç°ä»£æµè§ˆå™¨User-Agent

    ä»é¢„å®šä¹‰çš„ç°ä»£æµè§ˆå™¨User-Agentæ± ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªï¼Œç”¨äºæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è®¿é—®ã€‚
    åŒ…å«Chromeã€Firefoxã€Edgeç­‰ä¸»æµæµè§ˆå™¨çš„æœ€æ–°ç‰ˆæœ¬User-Agentã€‚

    Returns:
        str: éšæœºé€‰æ‹©çš„User-Agentå­—ç¬¦ä¸²
    """
    return random.choice(USER_AGENTS)

async def robust_page_goto(page, url: str, task_id: int, max_retries: int = 3,
                          wait_until: str = "domcontentloaded", timeout: int = 30000) -> bool:
    """
    å¢å¼ºçš„é¡µé¢å¯¼èˆªå‡½æ•°ï¼ŒåŒ…å«é‡è¯•é€»è¾‘å’Œé”™è¯¯å¤„ç†

    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        url: è¦è®¿é—®çš„URL
        task_id: ä»»åŠ¡IDï¼Œç”¨äºæ—¥å¿—è®°å½•
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        wait_until: ç­‰å¾…æ¡ä»¶
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        bool: æ˜¯å¦æˆåŠŸå¯¼èˆªåˆ°é¡µé¢
    """
    for attempt in range(max_retries):
        try:
            await log_to_database(task_id, 'INFO', f"å°è¯•è®¿é—®é¡µé¢ (ç¬¬{attempt + 1}/{max_retries}æ¬¡): {url[:100]}...")

            # ä½¿ç”¨é€Ÿç‡é™åˆ¶å™¨æ§åˆ¶è¯·æ±‚é¢‘ç‡
            await rate_limiter.wait_if_needed(task_id, log_to_database)

            # åœ¨é‡è¯•æ—¶æ·»åŠ é¢å¤–å»¶è¿Ÿ
            if attempt > 0:
                await log_to_database(task_id, 'INFO', f"é‡è¯•å‰å¢åŠ é¢å¤–å»¶è¿Ÿ...")
                await adaptive_sleep(5.0, 12.0, attempt, task_id, log_to_database)

            await page.goto(url, wait_until=wait_until, timeout=timeout)
            await log_to_database(task_id, 'INFO', f"æˆåŠŸè®¿é—®é¡µé¢: {url[:100]}...")

            # è®°å½•æˆåŠŸ
            rate_limiter.record_success()
            return True

        except Exception as e:
            error_str = str(e)
            await log_to_database(task_id, 'WARNING', f"é¡µé¢è®¿é—®å¤±è´¥ (ç¬¬{attempt + 1}/{max_retries}æ¬¡): {error_str}")

            # è®°å½•é”™è¯¯
            rate_limiter.record_error()

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯
            network_error_keywords = [
                "ERR_EMPTY_RESPONSE", "ERR_CONNECTION_RESET", "ERR_CONNECTION_REFUSED",
                "ERR_TIMED_OUT", "net::", "Protocol error", "Connection closed",
                "Timeout", "Connection reset", "Empty response", "Connection refused",
                "Target page, context or browser has been closed"
            ]

            is_network_error = any(err in error_str for err in network_error_keywords)

            if is_network_error:
                # æå–URLä¿¡æ¯
                url_info = url if 'url' in locals() else "æœªçŸ¥URL"

                # è¯¦ç»†çš„ç½‘ç»œé”™è¯¯ä¿¡æ¯
                error_details = {
                    "error_type": "network_error",
                    "error_message": error_str,
                    "target_url": url_info,
                    "attempt_number": attempt + 1,
                    "max_retries": max_retries,
                    "error_keywords": [kw for kw in network_error_keywords if kw in error_str]
                }

                await log_to_database(task_id, 'WARNING', f"ç½‘ç»œé”™è¯¯: {error_str} (URL: {url_info[:100]}...)")
                print(f"   [ç½‘ç»œé”™è¯¯] æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯: {error_str}")

                # ç«‹å³å°è¯•åˆ‡æ¢ä»£ç†
                if hasattr(robust_page_goto, '_current_context') and robust_page_goto._current_context:
                    await log_to_database(task_id, 'INFO', "ç½‘ç»œé”™è¯¯è§¦å‘ä»£ç†åˆ‡æ¢", {"trigger": "network_error"})
                    print(f"   [ç½‘ç»œé”™è¯¯] è§¦å‘ä»£ç†åˆ‡æ¢...")

                    # è¿™é‡Œéœ€è¦åœ¨è°ƒç”¨å¤„å¤„ç†ä»£ç†åˆ‡æ¢ï¼Œå› ä¸ºè¿™ä¸ªå‡½æ•°æ²¡æœ‰è®¿é—®browserå’Œcontext
                    # è¿”å›ç‰¹æ®Šæ ‡è¯†è®©è°ƒç”¨æ–¹çŸ¥é“éœ€è¦åˆ‡æ¢ä»£ç†
                    return "PROXY_SWITCH_NEEDED"

                if attempt < max_retries - 1:
                    retry_delay = random.randint(10, 20)
                    await log_to_database(task_id, 'INFO', f"ç½‘ç»œé”™è¯¯é‡è¯•å‰å»¶è¿Ÿ {retry_delay} ç§’", {
                        "delay_seconds": retry_delay,
                        "retry_reason": "network_error"
                    })
                    print(f"   [ç½‘ç»œé”™è¯¯] é‡è¯•å‰å¢åŠ é¢å¤–å»¶è¿Ÿ...")
                    await asyncio.sleep(retry_delay)
                    continue
                else:
                    await log_to_database(task_id, 'ERROR', f"ç½‘ç»œé”™è¯¯é‡è¯•å¤±è´¥: {error_str}", error_details)
                    return False
            else:
                # éç½‘ç»œé”™è¯¯ï¼Œç«‹å³å¤±è´¥
                await log_to_database(task_id, 'ERROR', f"é¡µé¢è®¿é—®å¤±è´¥: {error_str}")
                return False

    return False


async def robust_page_goto_with_proxy_switch(page, url: str, task_id: int, browser, context, proxy_address: str, max_retries: int = 3) -> tuple[bool, any, any, str]:
    """
    å¢å¼ºçš„é¡µé¢å¯¼èˆªå‡½æ•°ï¼Œæ”¯æŒç½‘ç»œé”™è¯¯æ—¶è‡ªåŠ¨åˆ‡æ¢ä»£ç†

    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        url: ç›®æ ‡URL
        task_id: ä»»åŠ¡ID
        browser: æµè§ˆå™¨å®ä¾‹
        context: æµè§ˆå™¨ä¸Šä¸‹æ–‡
        proxy_address: å½“å‰ä»£ç†åœ°å€
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        tuple: (æˆåŠŸæ ‡å¿—, æ–°çš„context, æ–°çš„page, æ–°çš„proxy_address)
    """
    current_context = context
    current_page = page
    current_proxy = proxy_address

    for attempt in range(max_retries):
        try:
            await log_to_database(task_id, 'INFO', f"å°è¯•è®¿é—®é¡µé¢ (ç¬¬{attempt + 1}/{max_retries}æ¬¡): {url}")

            await current_page.goto(url, wait_until="domcontentloaded", timeout=30000)
            await current_page.wait_for_load_state("networkidle", timeout=10000)

            await log_to_database(task_id, 'INFO', f"é¡µé¢è®¿é—®æˆåŠŸ")
            return True, current_context, current_page, current_proxy

        except Exception as e:
            error_str = str(e)
            await log_to_database(task_id, 'WARNING', f"é¡µé¢è®¿é—®å¤±è´¥ (ç¬¬{attempt + 1}/{max_retries}æ¬¡): {error_str}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯
            network_error_keywords = [
                "ERR_EMPTY_RESPONSE", "ERR_CONNECTION_RESET", "ERR_CONNECTION_REFUSED",
                "ERR_TIMED_OUT", "net::", "Protocol error", "Connection closed",
                "Timeout", "Connection reset", "Empty response", "Connection refused",
                "Target page, context or browser has been closed"
            ]

            is_network_error = any(err in error_str for err in network_error_keywords)

            if is_network_error and attempt < max_retries - 1:
                # è¯¦ç»†çš„ç½‘ç»œé”™è¯¯ä¿¡æ¯
                error_details = {
                    "error_type": "network_error_with_proxy_switch",
                    "error_message": error_str,
                    "target_url": url,
                    "attempt_number": attempt + 1,
                    "current_proxy": current_proxy
                }

                await log_to_database(task_id, 'WARNING', f"ç½‘ç»œé”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†: {error_str} (URL: {url[:100]}...)", error_details)
                print(f"   [ç½‘ç»œé”™è¯¯] æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†: {error_str}")

                # å°è¯•è·å–æ–°ä»£ç†
                new_proxy = await handle_proxy_failure(task_id)
                if new_proxy and new_proxy != current_proxy:
                    try:
                        # å…³é—­å½“å‰ä¸Šä¸‹æ–‡
                        await current_context.close()

                        # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡å’Œé¡µé¢
                        current_context = await create_browser_context(browser, new_proxy)
                        current_page = await current_context.new_page()
                        old_proxy = current_proxy
                        current_proxy = new_proxy

                        # è¯¦ç»†çš„ä»£ç†åˆ‡æ¢æˆåŠŸä¿¡æ¯
                        switch_details = {
                            "action": "proxy_switch_success",
                            "old_proxy": old_proxy,
                            "new_proxy": current_proxy,
                            "trigger_error": error_str,
                            "target_url": url
                        }

                        await log_to_database(task_id, 'INFO', f"ä»£ç†åˆ‡æ¢æˆåŠŸ: {old_proxy} -> {current_proxy}", switch_details)
                        print(f"   [ä»£ç†åˆ‡æ¢] æˆåŠŸåˆ‡æ¢åˆ°æ–°ä»£ç†: {current_proxy}")

                        # ç»§ç»­é‡è¯•
                        continue

                    except Exception as proxy_error:
                        switch_error_details = {
                            "action": "proxy_switch_failed",
                            "old_proxy": current_proxy,
                            "target_proxy": new_proxy,
                            "error_message": str(proxy_error),
                            "original_error": error_str
                        }

                        await log_to_database(task_id, 'ERROR', f"ä»£ç†åˆ‡æ¢å¤±è´¥: {str(proxy_error)}", switch_error_details)
                        print(f"   [ä»£ç†åˆ‡æ¢] ä»£ç†åˆ‡æ¢å¤±è´¥: {proxy_error}")

                # å¦‚æœæ— æ³•åˆ‡æ¢ä»£ç†ï¼Œå¢åŠ å»¶è¿Ÿåé‡è¯•
                retry_delay = random.randint(10, 20)
                await log_to_database(task_id, 'INFO', f"ä»£ç†åˆ‡æ¢åé‡è¯•å‰å»¶è¿Ÿ {retry_delay} ç§’", {
                    "delay_seconds": retry_delay,
                    "retry_reason": "proxy_switch_fallback"
                })
                print(f"   [ç½‘ç»œé”™è¯¯] é‡è¯•å‰å¢åŠ é¢å¤–å»¶è¿Ÿ...")
                await asyncio.sleep(retry_delay)

            elif not is_network_error:
                # éç½‘ç»œé”™è¯¯ï¼Œç«‹å³å¤±è´¥
                await log_to_database(task_id, 'ERROR', f"é¡µé¢è®¿é—®å¤±è´¥: {error_str}")
                return False, current_context, current_page, current_proxy

    await log_to_database(task_id, 'ERROR', f"é¡µé¢è®¿é—®é‡è¯•å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
    return False, current_context, current_page, current_proxy


# åˆ›å»ºå…¨å±€ä»£ç†ç®¡ç†å™¨å®ä¾‹ï¼ˆç¨åè®¾ç½®æ—¥å¿—ä¸Šä¸‹æ–‡ï¼‰
proxy_manager = ProxyManager(
    proxy_api_url=PROXY_API_URL,
    proxy_enabled=PROXY_ENABLED,
    refresh_interval=PROXY_REFRESH_INTERVAL,
    retry_count=PROXY_RETRY_COUNT
)

# å…¨å±€å®¢æˆ·ç«¯å˜é‡ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
client = None

def get_openai_client():
    """
    è·å–OpenAIå®¢æˆ·ç«¯å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰

    åªåœ¨éœ€è¦æ—¶æ‰åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼Œé¿å…åœ¨æ¨¡å—å¯¼å…¥æ—¶å°±è¦æ±‚é…ç½®å®Œæ•´ã€‚

    Returns:
        AsyncOpenAI: OpenAIå®¢æˆ·ç«¯å®ä¾‹

    Raises:
        SystemExit: å½“é…ç½®ä¸å®Œæ•´æˆ–åˆå§‹åŒ–å¤±è´¥æ—¶é€€å‡ºç¨‹åº
    """
    global client
    if client is None:
        # æ£€æŸ¥é…ç½®æ˜¯å¦é½å…¨
        if not all([BASE_URL, MODEL_NAME]):
            sys.exit("é”™è¯¯ï¼šè¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­å®Œæ•´è®¾ç½®äº† OPENAI_BASE_URL å’Œ OPENAI_MODEL_NAMEã€‚(OPENAI_API_KEY å¯¹äºæŸäº›æœåŠ¡æ˜¯å¯é€‰çš„)")

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        try:
            client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
        except Exception as e:
            sys.exit(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")

    return client

# åˆå§‹åŒ–æ•°æ®åº“
db = XianyuDatabase()

# å®šä¹‰ç›®å½•å’Œæ–‡ä»¶å
IMAGE_SAVE_DIR = "images"
os.makedirs(IMAGE_SAVE_DIR, exist_ok=True)

# å®šä¹‰ä¸‹è½½å›¾ç‰‡æ‰€éœ€çš„è¯·æ±‚å¤´
IMAGE_DOWNLOAD_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:139.0) Gecko/20100101 Firefox/139.0',
    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

def get_link_unique_key(link: str) -> str:
    """
    è·å–é“¾æ¥çš„å”¯ä¸€æ ‡è¯†é”®

    æˆªå–é“¾æ¥ä¸­ç¬¬ä¸€ä¸ª"&"ä¹‹å‰çš„å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†ä¾æ®ï¼Œç”¨äºåˆ¤æ–­å•†å“æ˜¯å¦å·²è¢«å¤„ç†ã€‚

    Args:
        link (str): å®Œæ•´çš„å•†å“é“¾æ¥URL

    Returns:
        str: é“¾æ¥çš„å”¯ä¸€æ ‡è¯†éƒ¨åˆ†
    """
    return link.split('&', 1)[0]

async def random_sleep(min_seconds: float, max_seconds: float):
    """
    å¼‚æ­¥éšæœºå»¶è¿Ÿå‡½æ•°

    åœ¨æŒ‡å®šèŒƒå›´å†…éšæœºç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œç”¨äºæ¨¡æ‹Ÿäººç±»æ“ä½œè¡Œä¸ºï¼Œé¿å…è¢«åçˆ¬è™«æœºåˆ¶æ£€æµ‹ã€‚

    Args:
        min_seconds (float): æœ€å°å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        max_seconds (float): æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
    """
    delay = random.uniform(min_seconds, max_seconds)
    print(f"   [å»¶è¿Ÿ] ç­‰å¾… {delay:.2f} ç§’... (èŒƒå›´: {min_seconds}-{max_seconds}s)") # è°ƒè¯•æ—¶å¯ä»¥å–æ¶ˆæ³¨é‡Š
    await asyncio.sleep(delay)



# å…¨å±€é€Ÿç‡é™åˆ¶å™¨
rate_limiter = RateLimiter()

async def save_to_database(data_record: dict, task_id: int):
    """å°†å•†å“å’ŒAIåˆ†ææ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ï¼Œæ›¿ä»£JSONLæ–‡ä»¶"""
    try:
        # åœ¨å•†å“æ•°æ®ä¸­æ·»åŠ task_id
        data_record['task_id'] = task_id
        
        # ä¿å­˜å•†å“ä¿¡æ¯
        product_db_id = await db.save_product(data_record)
        if not product_db_id:
            print(f"   [æ•°æ®åº“] å•†å“ä¿å­˜å¤±è´¥")
            return False
            
        # ä¿å­˜AIåˆ†æç»“æœ
        ai_analysis = data_record.get('ai_analysis', {})
        if ai_analysis:
            await db.save_ai_analysis(task_id, product_db_id, ai_analysis)
            print(f"   [æ•°æ®åº“] å•†å“å’ŒAIåˆ†æç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“")
        else:
            print(f"   [æ•°æ®åº“] å•†å“å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆæ— AIåˆ†æï¼‰")
            
        return True
    except Exception as e:
        print(f"   [æ•°æ®åº“] ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
        return False

async def calculate_reputation_from_ratings(ratings_json: list) -> dict:
    """ä»åŸå§‹è¯„ä»·APIæ•°æ®åˆ—è¡¨ä¸­ï¼Œè®¡ç®—ä½œä¸ºå–å®¶å’Œä¹°å®¶çš„å¥½è¯„æ•°ä¸å¥½è¯„ç‡ã€‚"""
    seller_total = 0
    seller_positive = 0
    buyer_total = 0
    buyer_positive = 0

    for card in ratings_json:
        # ä½¿ç”¨ safe_get ä¿è¯å®‰å…¨è®¿é—®
        data = await safe_get(card, 'cardData', default={})
        role_tag = await safe_get(data, 'rateTagList', 0, 'text', default='')
        rate_type = await safe_get(data, 'rate') # 1=å¥½è¯„, 0=ä¸­è¯„, -1=å·®è¯„

        if "å–å®¶" in role_tag:
            seller_total += 1
            if rate_type == 1:
                seller_positive += 1
        elif "ä¹°å®¶" in role_tag:
            buyer_total += 1
            if rate_type == 1:
                buyer_positive += 1

    # è®¡ç®—æ¯”ç‡ï¼Œå¹¶å¤„ç†é™¤ä»¥é›¶çš„æƒ…å†µ
    seller_rate = f"{(seller_positive / seller_total * 100):.2f}%" if seller_total > 0 else "N/A"
    buyer_rate = f"{(buyer_positive / buyer_total * 100):.2f}%" if buyer_total > 0 else "N/A"

    return {
        "ä½œä¸ºå–å®¶çš„å¥½è¯„æ•°": f"{seller_positive}/{seller_total}",
        "ä½œä¸ºå–å®¶çš„å¥½è¯„ç‡": seller_rate,
        "ä½œä¸ºä¹°å®¶çš„å¥½è¯„æ•°": f"{buyer_positive}/{buyer_total}",
        "ä½œä¸ºä¹°å®¶çš„å¥½è¯„ç‡": buyer_rate
    }

async def _parse_user_items_data(items_json: list) -> list:
    """è§£æç”¨æˆ·ä¸»é¡µçš„å•†å“åˆ—è¡¨APIçš„JSONæ•°æ®ã€‚"""
    parsed_list = []
    for card in items_json:
        data = card.get('cardData', {})
        status_code = data.get('itemStatus')
        if status_code == 0:
            status_text = "åœ¨å”®"
        elif status_code == 1:
            status_text = "å·²å”®"
        else:
            status_text = f"æœªçŸ¥çŠ¶æ€ ({status_code})"

        parsed_list.append({
            "å•†å“ID": data.get('id'),
            "å•†å“æ ‡é¢˜": data.get('title'),
            "å•†å“ä»·æ ¼": data.get('priceInfo', {}).get('price'),
            "å•†å“ä¸»å›¾": data.get('picInfo', {}).get('picUrl'),
            "å•†å“çŠ¶æ€": status_text
        })
    return parsed_list


async def scrape_user_profile(context, user_id: str) -> dict:
    """
    ã€æ–°ç‰ˆã€‘è®¿é—®æŒ‡å®šç”¨æˆ·çš„ä¸ªäººä¸»é¡µï¼ŒæŒ‰é¡ºåºé‡‡é›†å…¶æ‘˜è¦ä¿¡æ¯ã€å®Œæ•´çš„å•†å“åˆ—è¡¨å’Œå®Œæ•´çš„è¯„ä»·åˆ—è¡¨ã€‚
    """
    print(f"   -> å¼€å§‹é‡‡é›†ç”¨æˆ·ID: {user_id} çš„å®Œæ•´ä¿¡æ¯...")
    profile_data = {}
    page = await context.new_page()

    # ä¸ºå„é¡¹å¼‚æ­¥ä»»åŠ¡å‡†å¤‡Futureå’Œæ•°æ®å®¹å™¨
    head_api_future = asyncio.get_event_loop().create_future()

    all_items, all_ratings = [], []
    stop_item_scrolling, stop_rating_scrolling = asyncio.Event(), asyncio.Event()

    async def handle_response(response: Response):
        # æ•è·å¤´éƒ¨æ‘˜è¦API
        if "mtop.idle.web.user.page.head" in response.url and not head_api_future.done():
            try:
                head_api_future.set_result(await response.json())
                print(f"      [APIæ•è·] ç”¨æˆ·å¤´éƒ¨ä¿¡æ¯... æˆåŠŸ")
            except Exception as e:
                if not head_api_future.done(): head_api_future.set_exception(e)

        # æ•è·å•†å“åˆ—è¡¨API
        elif "mtop.idle.web.xyh.item.list" in response.url:
            try:
                data = await response.json()
                all_items.extend(data.get('data', {}).get('cardList', []))
                print(f"      [APIæ•è·] å•†å“åˆ—è¡¨... å½“å‰å·²æ•è· {len(all_items)} ä»¶")
                if not data.get('data', {}).get('nextPage', True):
                    stop_item_scrolling.set()
            except Exception as e:
                stop_item_scrolling.set()

        # æ•è·è¯„ä»·åˆ—è¡¨API
        elif "mtop.idle.web.trade.rate.list" in response.url:
            try:
                data = await response.json()
                all_ratings.extend(data.get('data', {}).get('cardList', []))
                print(f"      [APIæ•è·] è¯„ä»·åˆ—è¡¨... å½“å‰å·²æ•è· {len(all_ratings)} æ¡")
                if not data.get('data', {}).get('nextPage', True):
                    stop_rating_scrolling.set()
            except Exception as e:
                stop_rating_scrolling.set()

    page.on("response", handle_response)

    try:
        # --- ä»»åŠ¡1: å¯¼èˆªå¹¶é‡‡é›†å¤´éƒ¨ä¿¡æ¯ ---
        await page.goto(f"https://www.goofish.com/personal?userId={user_id}", wait_until="domcontentloaded", timeout=20000)
        head_data = await asyncio.wait_for(head_api_future, timeout=15)
        profile_data = await parse_user_head_data(head_data)

        # --- ä»»åŠ¡2: æ»šåŠ¨åŠ è½½æ‰€æœ‰å•†å“ (é»˜è®¤é¡µé¢) ---
        print("      [é‡‡é›†é˜¶æ®µ] å¼€å§‹é‡‡é›†è¯¥ç”¨æˆ·çš„å•†å“åˆ—è¡¨...")
        await random_sleep(2, 4) # ç­‰å¾…ç¬¬ä¸€é¡µå•†å“APIå®Œæˆ
        while not stop_item_scrolling.is_set():
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            try:
                await asyncio.wait_for(stop_item_scrolling.wait(), timeout=8)
            except asyncio.TimeoutError:
                print("      [æ»šåŠ¨è¶…æ—¶] å•†å“åˆ—è¡¨å¯èƒ½å·²åŠ è½½å®Œæ¯•ã€‚")
                break
        profile_data["å–å®¶å‘å¸ƒçš„å•†å“åˆ—è¡¨"] = await _parse_user_items_data(all_items)

        # --- ä»»åŠ¡3: ç‚¹å‡»å¹¶é‡‡é›†æ‰€æœ‰è¯„ä»· ---
        print("      [é‡‡é›†é˜¶æ®µ] å¼€å§‹é‡‡é›†è¯¥ç”¨æˆ·çš„è¯„ä»·åˆ—è¡¨...")
        rating_tab_locator = page.locator("//div[text()='ä¿¡ç”¨åŠè¯„ä»·']/ancestor::li")
        if await rating_tab_locator.count() > 0:
            await rating_tab_locator.click()
            await random_sleep(3, 5) # ç­‰å¾…ç¬¬ä¸€é¡µè¯„ä»·APIå®Œæˆ

            while not stop_rating_scrolling.is_set():
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                try:
                    await asyncio.wait_for(stop_rating_scrolling.wait(), timeout=8)
                except asyncio.TimeoutError:
                    print("      [æ»šåŠ¨è¶…æ—¶] è¯„ä»·åˆ—è¡¨å¯èƒ½å·²åŠ è½½å®Œæ¯•ã€‚")
                    break

            profile_data['å–å®¶æ”¶åˆ°çš„è¯„ä»·åˆ—è¡¨'] = await parse_ratings_data(all_ratings)
            reputation_stats = await calculate_reputation_from_ratings(all_ratings)
            profile_data.update(reputation_stats)
        else:
            print("      [è­¦å‘Š] æœªæ‰¾åˆ°è¯„ä»·é€‰é¡¹å¡ï¼Œè·³è¿‡è¯„ä»·é‡‡é›†ã€‚")

    except Exception as e:
        print(f"   [é”™è¯¯] é‡‡é›†ç”¨æˆ· {user_id} ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        page.remove_listener("response", handle_response)
        await page.close()
        print(f"   -> ç”¨æˆ· {user_id} ä¿¡æ¯é‡‡é›†å®Œæˆã€‚")

    return profile_data

async def parse_user_head_data(head_json: dict) -> dict:
    """è§£æç”¨æˆ·å¤´éƒ¨APIçš„JSONæ•°æ®ã€‚"""
    data = head_json.get('data', {})
    ylz_tags = await safe_get(data, 'module', 'base', 'ylzTags', default=[])
    seller_credit, buyer_credit = {}, {}
    for tag in ylz_tags:
        if await safe_get(tag, 'attributes', 'role') == 'seller':
            seller_credit = {'level': await safe_get(tag, 'attributes', 'level'), 'text': tag.get('text')}
        elif await safe_get(tag, 'attributes', 'role') == 'buyer':
            buyer_credit = {'level': await safe_get(tag, 'attributes', 'level'), 'text': tag.get('text')}
    return {
        "å–å®¶æ˜µç§°": await safe_get(data, 'module', 'base', 'displayName'),
        "å–å®¶å¤´åƒé“¾æ¥": await safe_get(data, 'module', 'base', 'avatar', 'avatar'),
        "å–å®¶ä¸ªæ€§ç­¾å": await safe_get(data, 'module', 'base', 'introduction', default=''),
        "å–å®¶åœ¨å”®/å·²å”®å•†å“æ•°": await safe_get(data, 'module', 'tabs', 'item', 'number'),
        "å–å®¶æ”¶åˆ°çš„è¯„ä»·æ€»æ•°": await safe_get(data, 'module', 'tabs', 'rate', 'number'),
        "å–å®¶ä¿¡ç”¨ç­‰çº§": seller_credit.get('text', 'æš‚æ— '),
        "ä¹°å®¶ä¿¡ç”¨ç­‰çº§": buyer_credit.get('text', 'æš‚æ— ')
    }


async def parse_ratings_data(ratings_json: list) -> list:
    """è§£æè¯„ä»·åˆ—è¡¨APIçš„JSONæ•°æ®ã€‚"""
    parsed_list = []
    for card in ratings_json:
        data = await safe_get(card, 'cardData', default={})
        rate_tag = await safe_get(data, 'rateTagList', 0, 'text', default='æœªçŸ¥è§’è‰²')
        rate_type = await safe_get(data, 'rate')
        if rate_type == 1: rate_text = "å¥½è¯„"
        elif rate_type == 0: rate_text = "ä¸­è¯„"
        elif rate_type == -1: rate_text = "å·®è¯„"
        else: rate_text = "æœªçŸ¥"
        parsed_list.append({
            "è¯„ä»·ID": data.get('rateId'),
            "è¯„ä»·å†…å®¹": data.get('feedback'),
            "è¯„ä»·ç±»å‹": rate_text,
            "è¯„ä»·æ¥æºè§’è‰²": rate_tag,
            "è¯„ä»·è€…æ˜µç§°": data.get('raterUserNick'),
            "è¯„ä»·æ—¶é—´": data.get('gmtCreate'),
            "è¯„ä»·å›¾ç‰‡": await safe_get(data, 'pictCdnUrlList', default=[])
        })
    return parsed_list

async def safe_get(data, *keys, default="æš‚æ— "):
    """
    å®‰å…¨è·å–åµŒå¥—å­—å…¸å€¼

    é€’å½’è®¿é—®åµŒå¥—å­—å…¸æˆ–åˆ—è¡¨ï¼Œå½“ä»»ä½•å±‚çº§çš„é”®ä¸å­˜åœ¨æ—¶è¿”å›é»˜è®¤å€¼ï¼Œé¿å…KeyErrorå¼‚å¸¸ã€‚
    æ”¯æŒå­—å…¸é”®è®¿é—®å’Œåˆ—è¡¨ç´¢å¼•è®¿é—®çš„æ··åˆä½¿ç”¨ã€‚

    Args:
        data: è¦è®¿é—®çš„æ•°æ®ç»“æ„ï¼ˆå­—å…¸ã€åˆ—è¡¨ç­‰ï¼‰
        *keys: è¦è®¿é—®çš„é”®åºåˆ—ï¼Œå¯ä»¥æ˜¯å­—å…¸é”®æˆ–åˆ—è¡¨ç´¢å¼•
        default: å½“è®¿é—®å¤±è´¥æ—¶è¿”å›çš„é»˜è®¤å€¼ï¼Œé»˜è®¤ä¸º"æš‚æ— "

    Returns:
        è®¿é—®åˆ°çš„å€¼æˆ–é»˜è®¤å€¼

    Example:
        await safe_get(data, 'user', 'profile', 'name', default='æœªçŸ¥ç”¨æˆ·')
        await safe_get(data, 'items', 0, 'title', default='æ— æ ‡é¢˜')
    """
    for key in keys:
        try:
            data = data[key]
        except (KeyError, TypeError, IndexError):
            return default
    return data

async def _parse_search_results_json(json_data: dict, source: str, task_id: int = None) -> list:
    """è§£ææœç´¢APIçš„JSONæ•°æ®ï¼Œè¿”å›åŸºç¡€å•†å“ä¿¡æ¯åˆ—è¡¨ã€‚"""
    page_data = []
    try:
        items = await safe_get(json_data, "data", "resultList", default=[])
        if not items:
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            items = await safe_get(json_data, "resultList", default=[])
            if not items:
                items = await safe_get(json_data, "data", "items", default=[])
                if not items:
                    debug_message = f"DEBUG: ({source}) å®Œæ•´JSONå“åº”: {json.dumps(json_data, ensure_ascii=False, indent=2)[:500]}..."
                    log_message = f"LOG: ({source}) APIå“åº”ä¸­æœªæ‰¾åˆ°å•†å“åˆ—è¡¨ (resultList)ã€‚"
                    print(debug_message)
                    print(log_message)

                    # è®°å½•åˆ°æ•°æ®åº“
                    if task_id:
                        await log_to_database(task_id, 'DEBUG', f"({source}) å®Œæ•´JSONå“åº”",
                                            {"json_preview": json.dumps(json_data, ensure_ascii=False, indent=2)[:500]})
                        await log_to_database(task_id, 'WARNING', f"({source}) APIå“åº”ä¸­æœªæ‰¾åˆ°å•†å“åˆ—è¡¨")
                    return []

        for item in items:
            main_data = await safe_get(item, "data", "item", "main", "exContent", default={})
            click_params = await safe_get(item, "data", "item", "main", "clickParam", "args", default={})

            title = await safe_get(main_data, "title", default="æœªçŸ¥æ ‡é¢˜")
            price_parts = await safe_get(main_data, "price", default=[])
            price = "".join([str(p.get("text", "")) for p in price_parts if isinstance(p, dict)]).replace("å½“å‰ä»·", "").strip() if isinstance(price_parts, list) else "ä»·æ ¼å¼‚å¸¸"
            if "ä¸‡" in price: price = f"Â¥{float(price.replace('Â¥', '').replace('ä¸‡', '')) * 10000:.0f}"
            area = await safe_get(main_data, "area", default="åœ°åŒºæœªçŸ¥")
            seller = await safe_get(main_data, "userNickName", default="åŒ¿åå–å®¶")
            raw_link = await safe_get(item, "data", "item", "main", "targetUrl", default="")
            image_url = await safe_get(main_data, "picUrl", default="")
            pub_time_ts = click_params.get("publishTime", "")
            item_id = await safe_get(main_data, "itemId", default="æœªçŸ¥ID")
            original_price = await safe_get(main_data, "oriPrice", default="æš‚æ— ")
            wants_count = await safe_get(click_params, "wantNum", default='NaN')


            tags = []
            if await safe_get(click_params, "tag") == "freeship":
                tags.append("åŒ…é‚®")
            r1_tags = await safe_get(main_data, "fishTags", "r1", "tagList", default=[])
            for tag_item in r1_tags:
                content = await safe_get(tag_item, "data", "content", default="")
                if "éªŒè´§å®" in content:
                    tags.append("éªŒè´§å®")

            page_data.append({
                "å•†å“æ ‡é¢˜": title,
                "å½“å‰å”®ä»·": price,
                "å•†å“åŸä»·": original_price,
                "â€œæƒ³è¦â€äººæ•°": wants_count,
                "å•†å“æ ‡ç­¾": tags,
                "å‘è´§åœ°åŒº": area,
                "å–å®¶æ˜µç§°": seller,
                "å•†å“é“¾æ¥": raw_link.replace("fleamarket://", "https://www.goofish.com/"),
                "å‘å¸ƒæ—¶é—´": datetime.fromtimestamp(int(pub_time_ts)/1000).strftime("%Y-%m-%d %H:%M") if pub_time_ts.isdigit() else "æœªçŸ¥æ—¶é—´",
                "å•†å“ID": item_id
            })
        print(f"LOG: ({source}) æˆåŠŸè§£æåˆ° {len(page_data)} æ¡å•†å“åŸºç¡€ä¿¡æ¯ã€‚")
        return page_data
    except Exception as e:
        print(f"LOG: ({source}) JSONæ•°æ®å¤„ç†å¼‚å¸¸: {str(e)}")
        return []

def format_registration_days(total_days: int) -> str:
    """
    å°†æ€»å¤©æ•°æ ¼å¼åŒ–ä¸ºâ€œXå¹´Yä¸ªæœˆâ€çš„å­—ç¬¦ä¸²ã€‚
    """
    if not isinstance(total_days, int) or total_days <= 0:
        return 'æœªçŸ¥'

    # ä½¿ç”¨æ›´ç²¾ç¡®çš„å¹³å‡å¤©æ•°
    DAYS_IN_YEAR = 365.25
    DAYS_IN_MONTH = DAYS_IN_YEAR / 12  # å¤§çº¦ 30.44

    # è®¡ç®—å¹´æ•°
    years = math.floor(total_days / DAYS_IN_YEAR)

    # è®¡ç®—å‰©ä½™å¤©æ•°
    remaining_days = total_days - (years * DAYS_IN_YEAR)

    # è®¡ç®—æœˆæ•°ï¼Œå››èˆäº”å…¥
    months = round(remaining_days / DAYS_IN_MONTH)

    # å¤„ç†è¿›ä½ï¼šå¦‚æœæœˆæ•°ç­‰äº12ï¼Œåˆ™å¹´æ•°åŠ 1ï¼Œæœˆæ•°å½’é›¶
    if months == 12:
        years += 1
        months = 0

    # æ„å»ºæœ€ç»ˆçš„è¾“å‡ºå­—ç¬¦ä¸²
    if years > 0 and months > 0:
        return f"æ¥é—²é±¼{years}å¹´{months}ä¸ªæœˆ"
    elif years > 0 and months == 0:
        return f"æ¥é—²é±¼{years}å¹´æ•´"
    elif years == 0 and months > 0:
        return f"æ¥é—²é±¼{months}ä¸ªæœˆ"
    else: # years == 0 and months == 0
        return "æ¥é—²é±¼ä¸è¶³ä¸€ä¸ªæœˆ"


# --- AIåˆ†æåŠé€šçŸ¥è¾…åŠ©å‡½æ•° (ä» ai_filter.py ç§»æ¤å¹¶å¼‚æ­¥åŒ–æ”¹é€ ) ---

def retry_on_failure(retries=3, delay=5):
    """
    ä¸€ä¸ªé€šç”¨çš„å¼‚æ­¥é‡è¯•è£…é¥°å™¨ï¼Œå¢åŠ äº†å¯¹HTTPé”™è¯¯çš„è¯¦ç»†æ—¥å¿—è®°å½•ã€‚
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for i in range(retries):
                try:
                    return await func(*args, **kwargs)
                except (APIStatusError, HTTPError) as e:
                    print(f"å‡½æ•° {func.__name__} ç¬¬ {i + 1}/{retries} æ¬¡å°è¯•å¤±è´¥ï¼Œå‘ç”ŸHTTPé”™è¯¯ã€‚")
                    if hasattr(e, 'status_code'):
                        print(f"  - çŠ¶æ€ç  (Status Code): {e.status_code}")
                    if hasattr(e, 'response') and hasattr(e.response, 'text'):
                        response_text = e.response.text
                        print(
                            f"  - è¿”å›å€¼ (Response): {response_text[:300]}{'...' if len(response_text) > 300 else ''}")
                except json.JSONDecodeError as e:
                    print(f"å‡½æ•° {func.__name__} ç¬¬ {i + 1}/{retries} æ¬¡å°è¯•å¤±è´¥: JSONè§£æé”™è¯¯ - {e}")
                except Exception as e:
                    print(f"å‡½æ•° {func.__name__} ç¬¬ {i + 1}/{retries} æ¬¡å°è¯•å¤±è´¥: {type(e).__name__} - {e}")

                if i < retries - 1:
                    print(f"å°†åœ¨ {delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)

            print(f"å‡½æ•° {func.__name__} åœ¨ {retries} æ¬¡å°è¯•åå½»åº•å¤±è´¥ã€‚")
            return None
        return wrapper
    return decorator


@retry_on_failure(retries=2, delay=3)
async def _download_single_image(url, save_path):
    """ä¸€ä¸ªå¸¦é‡è¯•çš„å†…éƒ¨å‡½æ•°ï¼Œç”¨äºå¼‚æ­¥ä¸‹è½½å•ä¸ªå›¾ç‰‡ã€‚"""
    loop = asyncio.get_running_loop()
    # ä½¿ç”¨ run_in_executor è¿è¡ŒåŒæ­¥çš„ requests ä»£ç ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
    response = await loop.run_in_executor(
        None,
        lambda: requests.get(url, headers=IMAGE_DOWNLOAD_HEADERS, timeout=20, stream=True)
    )
    response.raise_for_status()
    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    return save_path


async def download_all_images(product_id, image_urls):
    """
    æ‰¹é‡ä¸‹è½½å•†å“å›¾ç‰‡

    å¼‚æ­¥ä¸‹è½½æŒ‡å®šå•†å“çš„æ‰€æœ‰å›¾ç‰‡ï¼ŒæŒ‰å•†å“IDåˆ›å»ºç‹¬ç«‹ç›®å½•è¿›è¡Œç»„ç»‡ã€‚
    æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆè·³è¿‡å·²å­˜åœ¨çš„å›¾ç‰‡ï¼‰å’Œé”™è¯¯å¤„ç†ã€‚

    Args:
        product_id: å•†å“IDï¼Œç”¨äºåˆ›å»ºå›¾ç‰‡å­˜å‚¨ç›®å½•
        image_urls (list): å›¾ç‰‡URLåˆ—è¡¨

    Returns:
        list: æˆåŠŸä¸‹è½½çš„å›¾ç‰‡æœ¬åœ°è·¯å¾„åˆ—è¡¨
    """
    if not image_urls:
        return []

    urls = [url.strip() for url in image_urls if url.strip().startswith('http')]
    if not urls:
        return []

    # ä¸ºæ¯ä¸ªå•†å“åˆ›å»ºç‹¬ç«‹çš„å›¾ç‰‡ç›®å½•
    product_image_dir = os.path.join(IMAGE_SAVE_DIR, str(product_id))
    os.makedirs(product_image_dir, exist_ok=True)

    saved_paths = []
    total_images = len(urls)
    for i, url in enumerate(urls):
        try:
            clean_url = url.split('.heic')[0] if '.heic' in url else url
            file_name_base = os.path.basename(clean_url).split('?')[0]
            # æ–°çš„æ–‡ä»¶å‘½åæ ¼å¼ï¼šimage_{index}_{filename}
            file_name = f"image_{i + 1}_{file_name_base}"
            file_name = re.sub(r'[\\/*?:"<>|]', "", file_name)
            if not os.path.splitext(file_name)[1]:
                file_name += ".jpg"

            # æ–°çš„ä¿å­˜è·¯å¾„ï¼šimages/{product_id}/image_{index}_{filename}
            save_path = os.path.join(product_image_dir, file_name)

            if os.path.exists(save_path):
                print(f"   [å›¾ç‰‡] å›¾ç‰‡ {i + 1}/{total_images} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {os.path.basename(save_path)}")
                saved_paths.append(save_path)
                continue

            print(f"   [å›¾ç‰‡] æ­£åœ¨ä¸‹è½½å›¾ç‰‡ {i + 1}/{total_images}: {url}")
            if await _download_single_image(url, save_path):
                print(f"   [å›¾ç‰‡] å›¾ç‰‡ {i + 1}/{total_images} å·²æˆåŠŸä¸‹è½½åˆ°: {save_path}")
                saved_paths.append(save_path)
        except Exception as e:
            print(f"   [å›¾ç‰‡] å¤„ç†å›¾ç‰‡ {url} æ—¶å‘ç”Ÿé”™è¯¯ï¼Œå·²è·³è¿‡æ­¤å›¾: {e}")

    return saved_paths


def encode_image_to_base64(image_path):
    """
    å›¾ç‰‡Base64ç¼–ç å‡½æ•°

    å°†æœ¬åœ°å›¾ç‰‡æ–‡ä»¶è¯»å–å¹¶ç¼–ç ä¸ºBase64å­—ç¬¦ä¸²ï¼Œç”¨äºå‘é€ç»™AIæ¨¡å‹è¿›è¡Œå›¾åƒåˆ†æã€‚

    Args:
        image_path (str): æœ¬åœ°å›¾ç‰‡æ–‡ä»¶è·¯å¾„

    Returns:
        str: Base64ç¼–ç çš„å›¾ç‰‡å­—ç¬¦ä¸²ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ç¼–ç å¤±è´¥åˆ™è¿”å›None
    """
    if not image_path or not os.path.exists(image_path):
        return None
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        print(f"ç¼–ç å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        return None


@retry_on_failure(retries=3, delay=5)
async def send_ntfy_notification(product_data, reason):
    """
    å‘é€ntfyæ¨é€é€šçŸ¥

    å½“AIåˆ†æå‘ç°æ¨èå•†å“æ—¶ï¼Œé€šè¿‡ntfy.shæœåŠ¡å‘é€é«˜ä¼˜å…ˆçº§çš„æ¨é€é€šçŸ¥åˆ°ç”¨æˆ·è®¾å¤‡ã€‚
    æ”¯æŒè‡ªå®šä¹‰é€šçŸ¥æ ‡é¢˜ã€å†…å®¹å’Œä¼˜å…ˆçº§è®¾ç½®ã€‚

    Args:
        product_data (dict): å•†å“æ•°æ®å­—å…¸ï¼ŒåŒ…å«æ ‡é¢˜ã€ä»·æ ¼ã€é“¾æ¥ç­‰ä¿¡æ¯
        reason (str): æ¨èç†ç”±ï¼Œæ¥è‡ªAIåˆ†æç»“æœ

    Raises:
        Exception: å½“é€šçŸ¥å‘é€å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸ï¼ˆä¼šè¢«é‡è¯•è£…é¥°å™¨å¤„ç†ï¼‰
    """
    if not NTFY_TOPIC_URL:
        print("è­¦å‘Šï¼šæœªåœ¨ .env æ–‡ä»¶ä¸­é…ç½® NTFY_TOPIC_URLï¼Œè·³è¿‡é€šçŸ¥ã€‚")
        return

    title = product_data.get('å•†å“æ ‡é¢˜', 'N/A')
    price = product_data.get('å½“å‰å”®ä»·', 'N/A')
    link = product_data.get('å•†å“é“¾æ¥', '#')

    message = f"ä»·æ ¼: {price}\nåŸå› : {reason}\né“¾æ¥: {link}"
    notification_title = f"ğŸš¨ æ–°æ¨è! {title[:30]}..."

    try:
        print(f"   -> æ­£åœ¨å‘é€ ntfy é€šçŸ¥åˆ°: {NTFY_TOPIC_URL}")
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(
            None,
            lambda: requests.post(
                NTFY_TOPIC_URL,
                data=message.encode('utf-8'),
                headers={
                    "Title": notification_title.encode('utf-8'),
                    "Priority": "urgent",
                    "Tags": "bell,vibration"
                },
                timeout=10
            )
        )
        print("   -> é€šçŸ¥å‘é€æˆåŠŸã€‚")
    except Exception as e:
        print(f"   -> å‘é€ ntfy é€šçŸ¥å¤±è´¥: {e}")
        raise


@retry_on_failure(retries=5, delay=10)
async def get_ai_analysis(product_data, image_paths=None, prompt_text=""):
    """
    AIå•†å“åˆ†æå‡½æ•°

    å°†å®Œæ•´çš„å•†å“JSONæ•°æ®å’Œå•†å“å›¾ç‰‡å‘é€ç»™AIæ¨¡å‹è¿›è¡Œæ™ºèƒ½åˆ†æï¼Œ
    æ ¹æ®ç”¨æˆ·æä¾›çš„æç¤ºè¯åˆ¤æ–­å•†å“æ˜¯å¦ç¬¦åˆè´­ä¹°æ¡ä»¶ã€‚

    Args:
        product_data (dict): å®Œæ•´çš„å•†å“æ•°æ®å­—å…¸ï¼ŒåŒ…å«å•†å“ä¿¡æ¯ã€å–å®¶ä¿¡æ¯ç­‰
        image_paths (list, optional): å•†å“å›¾ç‰‡çš„æœ¬åœ°è·¯å¾„åˆ—è¡¨
        prompt_text (str): AIåˆ†æçš„æç¤ºè¯ï¼Œå®šä¹‰åˆ†ææ ‡å‡†å’Œè¾“å‡ºæ ¼å¼

    Returns:
        dict: AIåˆ†æç»“æœçš„JSONå¯¹è±¡ï¼ŒåŒ…å«æ¨èçŠ¶æ€ã€ç†ç”±ç­‰ä¿¡æ¯

    Raises:
        Exception: å½“AI APIè°ƒç”¨å¤±è´¥æˆ–å“åº”è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    item_info = product_data.get('å•†å“ä¿¡æ¯', {})
    product_id = item_info.get('å•†å“ID', 'N/A')

    print(f"\n   [AIåˆ†æ] å¼€å§‹åˆ†æå•†å“ #{product_id} (å« {len(image_paths or [])} å¼ å›¾ç‰‡)...")
    print(f"   [AIåˆ†æ] æ ‡é¢˜: {item_info.get('å•†å“æ ‡é¢˜', 'æ— ')}")

    if not prompt_text:
        print("   [AIåˆ†æ] é”™è¯¯ï¼šæœªæä¾›AIåˆ†ææ‰€éœ€çš„promptæ–‡æœ¬ã€‚")
        return None

    product_details_json = json.dumps(product_data, ensure_ascii=False, indent=2)
    system_prompt = prompt_text

    combined_text_prompt = f"""{system_prompt}

è¯·åŸºäºä½ çš„ä¸“ä¸šçŸ¥è¯†å’Œæˆ‘çš„è¦æ±‚ï¼Œåˆ†æä»¥ä¸‹å®Œæ•´çš„å•†å“JSONæ•°æ®ï¼š

```json
    {product_details_json}
"""
    user_content_list = [{"type": "text", "text": combined_text_prompt}]

    if image_paths:
        for path in image_paths:
            base64_image = encode_image_to_base64(path)
            if base64_image:
                user_content_list.append(
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}})

    messages = [{"role": "user", "content": user_content_list}]

    # ç¡®ä¿OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
    ai_client = get_openai_client()

    response = await ai_client.chat.completions.create(
        model=MODEL_NAME,
        messages=messages,
        response_format={"type": "json_object"}
    )

    ai_response_content = response.choices[0].message.content

    try:
        return json.loads(ai_response_content)
    except json.JSONDecodeError as e:
        print("---!!! AI RESPONSE PARSING FAILED (JSONDecodeError) !!!---")
        print(f"åŸå§‹è¿”å›å€¼ (Raw response from AI):\n---\n{ai_response_content}\n---")
        raise e


async def log_to_database(task_id: int, level: str, message: str, details: dict = None):
    """
    è®°å½•ä»»åŠ¡æ—¥å¿—åˆ°æ•°æ®åº“

    å°†ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­çš„é‡è¦äº‹ä»¶å’ŒçŠ¶æ€å˜åŒ–è®°å½•åˆ°æ•°æ®åº“ä¸­ï¼Œä¾¿äºåç»­åˆ†æå’Œè°ƒè¯•ã€‚
    æ”¯æŒä¸åŒçº§åˆ«çš„æ—¥å¿—è®°å½•ï¼ŒåŒ…æ‹¬INFOã€WARNINGã€ERRORç­‰ã€‚

    Args:
        task_id (int): ä»»åŠ¡IDï¼Œç”¨äºå…³è”æ—¥å¿—è®°å½•
        level (str): æ—¥å¿—çº§åˆ«ï¼Œå¦‚'INFO'ã€'WARNING'ã€'ERROR'
        message (str): æ—¥å¿—æ¶ˆæ¯å†…å®¹
        details (dict, optional): é¢å¤–çš„è¯¦ç»†ä¿¡æ¯ï¼Œä»¥å­—å…¸å½¢å¼å­˜å‚¨
    """
    try:
        await db.log_task_event(task_id, level, message, details)
    except Exception as e:
        print(f"è®°å½•æ•°æ®åº“æ—¥å¿—å¤±è´¥: {e}")


async def process_retry_products(retry_products, task_id: int, task_name: str, ai_prompt_text: str, email_enabled: bool, email_address: str):
    """
    å¤„ç†å¾…é‡è¯•è·å–è¯¦æƒ…çš„å•†å“

    Args:
        retry_products: å¾…é‡è¯•çš„å•†å“åˆ—è¡¨
        task_id: ä»»åŠ¡ID
        task_name: ä»»åŠ¡åç§°
        ai_prompt_text: AIåˆ†ææç¤ºè¯
        email_enabled: æ˜¯å¦å¯ç”¨é‚®ä»¶é€šçŸ¥
        email_address: é‚®ä»¶åœ°å€
    """
    await log_to_database(task_id, 'INFO', f"å¼€å§‹å¤„ç† {len(retry_products)} ä¸ªå¾…é‡è¯•å•†å“")
    print(f"\n=== å¼€å§‹å¤„ç†å¾…é‡è¯•å•†å“è¯¦æƒ… ===")

    retry_success_count = 0
    retry_fail_count = 0

    for product in retry_products:
        try:
            product_id = product['product_id']
            product_link = product['product_url']

            await log_to_database(task_id, 'INFO', f"é‡æ–°è·å–è¯¦æƒ…: {product['title'][:30]}...")
            print(f"   -> é‡æ–°è·å–å•†å“è¯¦æƒ…: {product['title'][:30]}...")

            # è¿™é‡Œåº”è¯¥å®ç°é‡æ–°è·å–è¯¦æƒ…çš„é€»è¾‘
            # ç”±äºéœ€è¦æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œæš‚æ—¶æ ‡è®°ä¸ºå¤„ç†ä¸­
            await db.update_product_detail_status(product_id, 'é‡è¯•ä¸­')

            # æ¨¡æ‹Ÿé‡æ–°è·å–è¯¦æƒ…çš„è¿‡ç¨‹
            # å®é™…å®ç°éœ€è¦åœ¨ä¸»çˆ¬è™«é€»è¾‘ä¸­é›†æˆ
            await log_to_database(task_id, 'INFO', f"å•†å“ {product_id} å·²æ ‡è®°ä¸ºé‡è¯•ä¸­ï¼Œå°†åœ¨ä¸»çˆ¬è™«æµç¨‹ä¸­å¤„ç†")

            retry_success_count += 1

        except Exception as e:
            await log_to_database(task_id, 'ERROR', f"å¤„ç†é‡è¯•å•†å“ {product.get('product_id', 'unknown')} å¤±è´¥: {str(e)}")
            print(f"   -> å¤„ç†é‡è¯•å•†å“å¤±è´¥: {e}")
            retry_fail_count += 1

    await log_to_database(task_id, 'INFO', f"é‡è¯•å¤„ç†å®Œæˆ: æˆåŠŸ {retry_success_count}, å¤±è´¥ {retry_fail_count}")
    print(f"=== é‡è¯•å¤„ç†å®Œæˆ: æˆåŠŸ {retry_success_count}, å¤±è´¥ {retry_fail_count} ===")



# åˆå§‹åŒ–Cookieç®¡ç†å™¨
cookie_manager = CookieManager(db)

async def create_browser_context(browser, proxy_address: Optional[str] = None):
    """
    åˆ›å»ºé…ç½®å®Œæ•´çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡

    ä½¿ç”¨Cookieæ± å’Œä»£ç†æ± åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«ç™»å½•çŠ¶æ€ã€éšæœºUser-Agentå’Œä»£ç†é…ç½®ã€‚
    è¿™æ˜¯çˆ¬è™«ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œç¡®ä¿æ¯ä¸ªè¯·æ±‚éƒ½æœ‰åˆé€‚çš„èº«ä»½å’Œç½‘ç»œé…ç½®ã€‚

    Args:
        browser: Playwrightæµè§ˆå™¨å®ä¾‹
        proxy_address (Optional[str]): ä»£ç†åœ°å€ï¼Œæ ¼å¼ä¸º"ip:port"ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä½¿ç”¨ä»£ç†

    Returns:
        BrowserContext: é…ç½®å®Œæ•´çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡å¯¹è±¡

    Raises:
        Exception: å½“æ— å¯ç”¨Cookieæ—¶æŠ›å‡ºå¼‚å¸¸
    """
    # è·å–å¯ç”¨Cookie
    cookie_data = await cookie_manager.get_available_cookie()

    if not cookie_data:
        raise Exception("æ— å¯ç”¨Cookieï¼Œè¯·å…ˆæ·»åŠ Cookie")

    # ä½¿ç”¨éšæœºçš„ç°ä»£æµè§ˆå™¨User-Agent
    user_agent = get_random_user_agent()
    context_options = {
        'storage_state': cookie_data,
        'user_agent': user_agent
    }
    print(f"   [æµè§ˆå™¨] ä½¿ç”¨User-Agent: {user_agent}")

    if proxy_address:
        context_options['proxy'] = {
            'server': f"http://{proxy_address}"
        }
        print(f"   [ä»£ç†] ä½¿ç”¨ä»£ç†: {proxy_address}")
        proxy_manager.record_usage()
    else:
        print("   [ä»£ç†] ä¸ä½¿ç”¨ä»£ç†")

    return await browser.new_context(**context_options)

async def scrape_xianyu(task_config: dict, debug_limit: int = 0):
    """
    é—²é±¼å•†å“çˆ¬å–æ ¸å¿ƒæ‰§è¡Œå™¨

    æ ¹æ®ä»»åŠ¡é…ç½®å¼‚æ­¥çˆ¬å–é—²é±¼å•†å“æ•°æ®ï¼Œæ”¯æŒå¤šé¡µçˆ¬å–ã€å®æ—¶AIåˆ†æã€æ™ºèƒ½é€šçŸ¥æ¨é€ã€‚
    åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†ã€ä»£ç†è½®æ¢ã€Cookieç®¡ç†ç­‰åŠŸèƒ½ã€‚

    Args:
        task_config (dict): ä»»åŠ¡é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - task_id (int, optional): ä»»åŠ¡ID
            - keyword (str): æœç´¢å…³é”®è¯
            - task_name (str): ä»»åŠ¡åç§°
            - max_pages (int, optional): æœ€å¤§çˆ¬å–é¡µæ•°ï¼Œé»˜è®¤1
            - personal_only (bool, optional): æ˜¯å¦åªçˆ¬å–ä¸ªäººå•†å“ï¼Œé»˜è®¤False
            - min_price (int, optional): æœ€ä½ä»·æ ¼ç­›é€‰
            - max_price (int, optional): æœ€é«˜ä»·æ ¼ç­›é€‰
            - ai_prompt_text (str, optional): AIåˆ†ææç¤ºè¯
            - email_enabled (bool, optional): æ˜¯å¦å¯ç”¨é‚®ä»¶é€šçŸ¥
            - email_address (str, optional): é‚®ä»¶æ¥æ”¶åœ°å€
        debug_limit (int, optional): è°ƒè¯•æ¨¡å¼ä¸‹çš„å•†å“å¤„ç†æ•°é‡é™åˆ¶ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶

    Returns:
        int: æœ¬æ¬¡è¿è¡Œå¤„ç†çš„æ–°å•†å“æ•°é‡

    Raises:
        Exception: å½“æ— æ³•æ‰¾åˆ°ä»»åŠ¡IDæˆ–åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    keyword = task_config['keyword']
    task_id = task_config['task_id']
    task_name = task_config['task_name']
    max_pages = task_config.get('max_pages', 1)
    personal_only = task_config.get('personal_only', False)
    min_price = task_config.get('min_price')
    max_price = task_config.get('max_price')
    ai_prompt_text = task_config.get('ai_prompt_text', '')
    
    # é‚®ä»¶é€šçŸ¥é…ç½®
    email_enabled = task_config.get('email_enabled', False)
    email_address = task_config.get('email_address', '')

    await log_to_database(task_id, 'INFO', f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_name}")

    # è®¾ç½®ä»£ç†ç®¡ç†å™¨å’ŒCookieç®¡ç†å™¨çš„æ—¥å¿—ä¸Šä¸‹æ–‡
    proxy_manager.set_log_context(log_to_database, task_id)
    cookie_manager.set_log_context(log_to_database, task_id)

    # æ£€æŸ¥é‚®ä»¶é…ç½®
    if email_enabled and email_address:
        if email_sender.is_configured():
            await log_to_database(task_id, 'INFO', f"é‚®ä»¶é€šçŸ¥å·²å¯ç”¨: {email_address}")
        else:
            await log_to_database(task_id, 'WARNING', "é‚®ä»¶é€šçŸ¥å·²å¯ç”¨ä½†SMTPé…ç½®ä¸å®Œæ•´")
            print(f"   [é‚®ä»¶] ä»»åŠ¡ '{task_name}' å¯ç”¨äº†é‚®ä»¶é€šçŸ¥ï¼Œä½†SMTPé…ç½®ä¸å®Œæ•´")

    processed_item_count = 0
    stop_scraping = False

    # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„å•†å“é“¾æ¥
    processed_links = await db.get_processed_product_links(task_id)
    print(f"LOG: ä»æ•°æ®åº“åŠ è½½äº† {len(processed_links)} ä¸ªå·²å¤„ç†è¿‡çš„å•†å“ã€‚")
    await log_to_database(task_id, 'INFO', f"ä»æ•°æ®åº“åŠ è½½äº† {len(processed_links)} ä¸ªå·²å¤„ç†è¿‡çš„å•†å“")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        
        # è·å–åˆå§‹ä»£ç†åœ°å€
        proxy_address = await get_proxy_with_fallback()
        if proxy_address:
            await log_to_database(task_id, 'INFO', f"ä½¿ç”¨ä»£ç†: {proxy_address}")
            await log_proxy_stats(task_id)
        else:
            await log_to_database(task_id, 'INFO', "ä¸ä½¿ç”¨ä»£ç†")
        
        # åˆ›å»ºå¸¦ä»£ç†å’ŒCookieæ± çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡
        try:
            context = await create_browser_context(browser, proxy_address)
        except Exception as e:
            await log_to_database(task_id, 'ERROR', f"åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
            print(f"LOG: åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return 0
            
        page = await context.new_page()

        try:
            # æ„å»ºæœç´¢URL
            search_url = f"https://www.goofish.com/search?q={keyword}"
            if personal_only:
                search_url += "&st=1"
            if min_price:
                search_url += f"&price_start={min_price}"
            if max_price:
                search_url += f"&price_end={max_price}"

            await log_to_database(task_id, 'INFO', f"å¼€å§‹æœç´¢: {search_url}")
            print(f"LOG: ä»»åŠ¡ '{task_name}' å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")

            # è®¿é—®æœç´¢é¡µé¢å¹¶ç­‰å¾…APIå“åº”
            try:
                async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=30000) as response_info:
                    # ä½¿ç”¨å¢å¼ºçš„é¡µé¢å¯¼èˆªå‡½æ•°
                    navigation_success = await robust_page_goto(page, search_url, task_id, max_retries=3)
                    if not navigation_success:
                        raise Exception("é¡µé¢å¯¼èˆªå¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                current_response = await response_info.value
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯Cookieé—®é¢˜
                if "ç™»å½•" in str(e) or "éªŒè¯" in str(e):
                    await log_to_database(task_id, 'WARNING', "æ£€æµ‹åˆ°Cookieå¯èƒ½å¤±æ•ˆï¼Œå°è¯•åˆ‡æ¢Cookie")
                    print("   [Cookieç®¡ç†] æ£€æµ‹åˆ°å¯èƒ½çš„Cookieé—®é¢˜ï¼Œå°è¯•åˆ‡æ¢")
                    
                    # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªCookie
                    new_cookie_data = await cookie_manager.switch_to_next_cookie()
                    if new_cookie_data:
                        await context.close()
                        context = await create_browser_context(browser, proxy_address)
                        page = await context.new_page()
                        
                        # é‡è¯•è®¿é—®
                        try:
                            async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=30000) as response_info:
                                # ä½¿ç”¨å¢å¼ºçš„é¡µé¢å¯¼èˆªå‡½æ•°
                                navigation_success = await robust_page_goto(page, search_url, task_id, max_retries=2)
                                if not navigation_success:
                                    raise Exception("Cookieåˆ‡æ¢åé¡µé¢å¯¼èˆªä»ç„¶å¤±è´¥")
                            current_response = await response_info.value
                        except Exception as retry_e:
                            await log_to_database(task_id, 'ERROR', f"åˆ‡æ¢Cookieåä»ç„¶å¤±è´¥: {str(retry_e)}")
                            raise retry_e
                    else:
                        await log_to_database(task_id, 'ERROR', "æ— æ›´å¤šå¯ç”¨Cookie")
                        raise Exception("æ— å¯ç”¨Cookie")
                else:
                    raise e

            if not current_response.ok:
                error_msg = f"æœç´¢APIå“åº”å¤±è´¥: {current_response.status}"
                await log_to_database(task_id, 'ERROR', error_msg)
                print(f"LOG: {error_msg}")
                return 0

            await log_to_database(task_id, 'INFO', f"æˆåŠŸè·å–æœç´¢APIå“åº”")

            # å¤„ç†å¤šé¡µæ•°æ®
            for page_num in range(1, max_pages + 1):
                if stop_scraping:
                    break

                # åœ¨æ¯é¡µå¼€å§‹å‰æ£€æŸ¥ä»£ç†çŠ¶æ€
                current_proxy = await get_proxy_with_fallback()
                if current_proxy != proxy_address:
                    await log_to_database(task_id, 'INFO', f"ä»£ç†å·²è‡ªåŠ¨æ›´æ¢: {proxy_address} -> {current_proxy}")
                    print(f"   [ä»£ç†ç®¡ç†] ä»£ç†å·²è‡ªåŠ¨æ›´æ¢: {proxy_address} -> {current_proxy}")
                    # é‡æ–°åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
                    await context.close()
                    context = await create_browser_context(browser, current_proxy)
                    page = await context.new_page()
                    proxy_address = current_proxy
                    await log_proxy_stats(task_id)

                await log_to_database(task_id, 'INFO', f"å¼€å§‹å¤„ç†ç¬¬ {page_num} é¡µ")
                print(f"LOG: æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...")

                if page_num > 1:
                    # ç¿»é¡µå‰å†æ¬¡æ£€æŸ¥ä»£ç†
                    current_proxy = await get_proxy_with_fallback()
                    if current_proxy != proxy_address:
                        await log_to_database(task_id, 'INFO', f"ç¿»é¡µå‰ä»£ç†æ›´æ¢: {proxy_address} -> {current_proxy}")
                        print(f"   [ä»£ç†ç®¡ç†] ç¿»é¡µå‰ä»£ç†æ›´æ¢: {proxy_address} -> {current_proxy}")
                        await context.close()
                        context = await create_browser_context(browser, current_proxy)
                        page = await context.new_page()
                        proxy_address = current_proxy
                        await log_proxy_stats(task_id)
                    
                    # ç¿»é¡µé€»è¾‘
                    await random_sleep(5, 10)
                    next_page_url = f"{search_url}&page={page_num}"
                    
                    try:
                        async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=30000) as response_info:
                            # ä½¿ç”¨å¢å¼ºçš„é¡µé¢å¯¼èˆªå‡½æ•°è¿›è¡Œç¿»é¡µ
                            navigation_success = await robust_page_goto(page, next_page_url, task_id, max_retries=2)
                            if not navigation_success:
                                raise Exception("ç¿»é¡µå¯¼èˆªå¤±è´¥")
                        current_response = await response_info.value
                    except Exception as e:
                        # ç½‘ç»œé”™è¯¯æ—¶ç«‹å³å°è¯•æ›´æ¢ä»£ç†
                        await log_to_database(task_id, 'WARNING', f"ç¿»é¡µæ—¶ç½‘ç»œé”™è¯¯ï¼Œå°è¯•æ›´æ¢ä»£ç†: {str(e)}")
                        print(f"   [ç½‘ç»œé”™è¯¯] ç¿»é¡µå¤±è´¥ï¼Œç«‹å³å°è¯•æ›´æ¢ä»£ç†: {e}")
                        new_proxy = await handle_proxy_failure(task_id)
                        if new_proxy and new_proxy != proxy_address:
                            await context.close()
                            context = await create_browser_context(browser, new_proxy)
                            page = await context.new_page()
                            proxy_address = new_proxy
                            # é‡è¯•ç¿»é¡µ
                            try:
                                async with page.expect_response(lambda r: API_URL_PATTERN in r.url, timeout=30000) as response_info:
                                    # ä½¿ç”¨å¢å¼ºçš„é¡µé¢å¯¼èˆªå‡½æ•°é‡è¯•ç¿»é¡µ
                                    navigation_success = await robust_page_goto(page, next_page_url, task_id, max_retries=2)
                                    if not navigation_success:
                                        raise Exception("ä»£ç†æ›´æ¢åç¿»é¡µå¯¼èˆªä»ç„¶å¤±è´¥")
                                current_response = await response_info.value
                            except Exception as retry_e:
                                await log_to_database(task_id, 'ERROR', f"æ›´æ¢ä»£ç†åç¿»é¡µä»å¤±è´¥: {str(retry_e)}")
                                print(f"   [ç½‘ç»œé”™è¯¯] æ›´æ¢ä»£ç†åç¿»é¡µä»å¤±è´¥: {retry_e}")
                                continue
                        else:
                            continue

                if not (current_response and current_response.ok):
                    await log_to_database(task_id, 'WARNING', f"ç¬¬ {page_num} é¡µå“åº”æ— æ•ˆï¼Œè·³è¿‡")
                    print(f"LOG: ç¬¬ {page_num} é¡µå“åº”æ— æ•ˆï¼Œè·³è¿‡ã€‚")
                    continue

                basic_items = await _parse_search_results_json(await current_response.json(), f"ç¬¬ {page_num} é¡µ", task_id)
                if not basic_items: 
                    await log_to_database(task_id, 'INFO', f"ç¬¬ {page_num} é¡µæ²¡æœ‰å•†å“æ•°æ®")
                    break

                await log_to_database(task_id, 'INFO', f"ç¬¬ {page_num} é¡µè§£æåˆ° {len(basic_items)} ä¸ªå•†å“")

                total_items_on_page = len(basic_items)
                for i, item_data in enumerate(basic_items, 1):
                    if debug_limit > 0 and processed_item_count >= debug_limit:
                        await log_to_database(task_id, 'INFO', f"å·²è¾¾åˆ°è°ƒè¯•ä¸Šé™ ({debug_limit})ï¼Œåœæ­¢è·å–æ–°å•†å“")
                        print(f"LOG: å·²è¾¾åˆ°è°ƒè¯•ä¸Šé™ ({debug_limit})ï¼Œåœæ­¢è·å–æ–°å•†å“ã€‚")
                        stop_scraping = True
                        break

                    unique_key = get_link_unique_key(item_data["å•†å“é“¾æ¥"])
                    
                    # æ£€æŸ¥å•†å“æ˜¯å¦å·²å­˜åœ¨çš„é€»è¾‘
                    if unique_key in processed_links:
                        if SKIP_EXISTING_PRODUCTS:
                            await log_to_database(task_id, 'INFO', f"å•†å“å·²å­˜åœ¨ï¼Œæ ¹æ®é…ç½®è·³è¿‡: {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                            print(f"   -> [é¡µå†…è¿›åº¦ {i}/{total_items_on_page}] å•†å“ '{item_data['å•†å“æ ‡é¢˜'][:20]}...' å·²å­˜åœ¨ï¼Œæ ¹æ®é…ç½®è·³è¿‡ã€‚")
                            continue
                        else:
                            await log_to_database(task_id, 'INFO', f"å•†å“å·²å­˜åœ¨ï¼Œä½†é…ç½®ä¸ºé‡æ–°è·å–: {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                            print(f"   -> [é¡µå†…è¿›åº¦ {i}/{total_items_on_page}] å•†å“ '{item_data['å•†å“æ ‡é¢˜'][:20]}...' å·²å­˜åœ¨ï¼Œä½†å°†é‡æ–°è·å–è¯¦æƒ…ã€‚")

                    await log_to_database(task_id, 'INFO', f"å‘ç°æ–°å•†å“: {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                    print(f"-> [é¡µå†…è¿›åº¦ {i}/{total_items_on_page}] {'å‘ç°æ–°å•†å“' if unique_key not in processed_links else 'é‡æ–°è·å–å•†å“'}ï¼Œè·å–è¯¦æƒ…: {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                    
                    # è®¿é—®è¯¦æƒ…é¡µå‰çš„ç­‰å¾…æ—¶é—´
                    await random_sleep(3, 6)

                    detail_page = await context.new_page()
                    detail_fetch_success = False
                    detail_retry_count = 0
                    max_detail_retries = 3
                    
                    while not detail_fetch_success and detail_retry_count < max_detail_retries:
                        try:
                            detail_retry_count += 1
                            if detail_retry_count > 1:
                                await log_to_database(task_id, 'INFO', f"é‡è¯•è·å–å•†å“è¯¦æƒ… (ç¬¬{detail_retry_count}æ¬¡): {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                                print(f"   -> é‡è¯•è·å–å•†å“è¯¦æƒ… (ç¬¬{detail_retry_count}æ¬¡)...")
                                # é‡è¯•å‰æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ¢ä»£ç†
                                current_proxy = await get_proxy_with_fallback()
                                if current_proxy != proxy_address:
                                    await log_to_database(task_id, 'INFO', f"é‡è¯•å‰æ›´æ¢ä»£ç†: {proxy_address} -> {current_proxy}")
                                    print(f"   [ä»£ç†ç®¡ç†] é‡è¯•å‰æ›´æ¢ä»£ç†: {proxy_address} -> {current_proxy}")
                                    await context.close()
                                    context = await create_browser_context(browser, current_proxy)
                                    page = await context.new_page()
                                    detail_page = await context.new_page()
                                    proxy_address = current_proxy
                                    await log_proxy_stats(task_id)
                                
                                # é‡è¯•å‰çš„æŒ‡æ•°é€€é¿å»¶è¿Ÿ
                                retry_delay = min(10 * (2 ** (detail_retry_count - 2)), 60)
                                await asyncio.sleep(retry_delay)
                            
                            async with detail_page.expect_response(lambda r: DETAIL_API_URL_PATTERN in r.url, timeout=25000) as detail_info:
                                # ä½¿ç”¨å¢å¼ºçš„é¡µé¢å¯¼èˆªå‡½æ•°è®¿é—®å•†å“è¯¦æƒ…é¡µ
                                navigation_success = await robust_page_goto(detail_page, item_data["å•†å“é“¾æ¥"], task_id, max_retries=2, timeout=25000)
                                if not navigation_success:
                                    raise Exception("å•†å“è¯¦æƒ…é¡µå¯¼èˆªå¤±è´¥")

                            detail_response = await detail_info.value
                            if detail_response.ok:
                                detail_json = await detail_response.json()

                                ret_string = str(await safe_get(detail_json, 'ret', default=[]))
                                if "FAIL_SYS_USER_VALIDATE" in ret_string:
                                    print("\n==================== CRITICAL BLOCK DETECTED ====================")
                                    print("æ£€æµ‹åˆ°é—²é±¼åçˆ¬è™«éªŒè¯ (FAIL_SYS_USER_VALIDATE)ï¼Œå°è¯•æ›´æ¢ä»£ç†...")
                                    
                                    # ç«‹å³å°è¯•è·å–æ–°ä»£ç†
                                    new_proxy = await handle_proxy_failure(task_id)
                                    if new_proxy and new_proxy != proxy_address:
                                        print(f"   [ä»£ç†] æ›´æ¢ä¸ºæ–°ä»£ç†: {new_proxy}")
                                        await context.close()
                                        context = await create_browser_context(browser, new_proxy)
                                        page = await context.new_page()
                                        proxy_address = new_proxy
                                        await log_proxy_stats(task_id)
                                        continue
                                    else:
                                        await log_to_database(task_id, 'ERROR', "æ— æ³•è·å–æ–°ä»£ç†ï¼Œæ‰§è¡Œé•¿æ—¶é—´ä¼‘çœ åé€€å‡º")
                                        print("   [ä»£ç†] æ— æ³•è·å–æ–°ä»£ç†ï¼Œæ‰§è¡Œé•¿æ—¶é—´ä¼‘çœ ...")
                                        long_sleep_duration = random.randint(300, 600)
                                        print(f"ä¸ºé¿å…è´¦æˆ·é£é™©ï¼Œå°†æ‰§è¡Œä¸€æ¬¡é•¿æ—¶é—´ä¼‘çœ  ({long_sleep_duration} ç§’) åå†é€€å‡º...")
                                        await asyncio.sleep(long_sleep_duration)
                                        print("é•¿æ—¶é—´ä¼‘çœ ç»“æŸï¼Œç°åœ¨å°†å®‰å…¨é€€å‡ºã€‚")
                                        print("===================================================================")
                                        stop_scraping = True
                                        break

                                # è§£æå•†å“è¯¦æƒ…æ•°æ®å¹¶æ›´æ–° item_data
                                item_do = await safe_get(detail_json, 'data', 'itemDO', default={})
                                seller_do = await safe_get(detail_json, 'data', 'sellerDO', default={})

                                reg_days_raw = await safe_get(seller_do, 'userRegDay', default=0)
                                registration_duration_text = format_registration_days(reg_days_raw)

                                # 1. æå–å–å®¶çš„èŠéº»ä¿¡ç”¨ä¿¡æ¯
                                zhima_credit_text = await safe_get(seller_do, 'zhimaLevelInfo', 'levelName')

                                # 2. æå–è¯¥å•†å“çš„å®Œæ•´å›¾ç‰‡åˆ—è¡¨
                                image_infos = await safe_get(item_do, 'imageInfos', default=[])
                                if image_infos:
                                    all_image_urls = [img.get('url') for img in image_infos if img.get('url')]
                                    if all_image_urls:
                                        item_data['å•†å“å›¾ç‰‡åˆ—è¡¨'] = all_image_urls
                                        item_data['å•†å“ä¸»å›¾é“¾æ¥'] = all_image_urls[0]

                                item_data['"æƒ³è¦"äººæ•°'] = await safe_get(item_do, 'wantCnt', default=item_data.get('"æƒ³è¦"äººæ•°', 'NaN'))
                                item_data['æµè§ˆé‡'] = await safe_get(item_do, 'browseCnt', default='-')

                                # è°ƒç”¨æ ¸å¿ƒå‡½æ•°é‡‡é›†å–å®¶ä¿¡æ¯
                                user_profile_data = {}
                                user_id = await safe_get(seller_do, 'sellerId')
                                if user_id:
                                    user_profile_data = await scrape_user_profile(context, str(user_id))
                                else:
                                    print("   [è­¦å‘Š] æœªèƒ½ä»è¯¦æƒ…APIä¸­è·å–åˆ°å–å®¶IDã€‚")
                                user_profile_data['å–å®¶èŠéº»ä¿¡ç”¨'] = zhima_credit_text
                                user_profile_data['å–å®¶æ³¨å†Œæ—¶é•¿'] = registration_duration_text

                                detail_fetch_success = True
                                await log_to_database(task_id, 'INFO', f"æˆåŠŸè·å–å•†å“è¯¦æƒ…: {item_data['å•†å“æ ‡é¢˜'][:30]}...")
                                
                            else:
                                error_msg = f"è¯¦æƒ…é¡µAPIå“åº”å¤±è´¥: HTTP {detail_response.status}"
                                await log_to_database(task_id, 'WARNING', error_msg)
                                print(f"   -> {error_msg}")
                                if detail_retry_count >= max_detail_retries:
                                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ•°æ®
                                    user_profile_data = {"è·å–çŠ¶æ€": "è¯¦æƒ…é¡µè®¿é—®å¤±è´¥"}
                                    detail_fetch_success = True  # æ ‡è®°ä¸ºæˆåŠŸä»¥ç»§ç»­å¤„ç†
                                
                        except PlaywrightTimeoutError as e:
                            error_msg = f"è®¿é—®å•†å“è¯¦æƒ…é¡µè¶…æ—¶ (ç¬¬{detail_retry_count}æ¬¡å°è¯•)"
                            await log_to_database(task_id, 'WARNING', error_msg)
                            print(f"   -> {error_msg}: {str(e)}")
                            
                            if detail_retry_count >= max_detail_retries:
                                # è¶…æ—¶é‡è¯•æ¬¡æ•°ç”¨å®Œï¼Œä½¿ç”¨åŸºç¡€æ•°æ®ç»§ç»­
                                user_profile_data = {"è·å–çŠ¶æ€": "è¯¦æƒ…é¡µè®¿é—®è¶…æ—¶"}
                                detail_fetch_success = True
                                
                        except Exception as e:
                            error_str = str(e)
                            
                            # è¯†åˆ«ç½‘ç»œçº§åˆ«é”™è¯¯
                            is_network_error = any(keyword in error_str.lower() for keyword in [
                                'net::err_empty_response', 'net::err_connection_reset', 
                                'net::err_connection_refused', 'net::err_timed_out',
                                'connection reset', 'empty response', 'connection refused'
                            ])
                            
                            if is_network_error:
                                # è¯¦ç»†çš„å•†å“è¯¦æƒ…ç½‘ç»œé”™è¯¯ä¿¡æ¯
                                detail_error_info = {
                                    "error_type": "product_detail_network_error",
                                    "error_message": error_str,
                                    "product_id": item_data.get('å•†å“ID', 'unknown'),
                                    "product_title": item_data.get('å•†å“æ ‡é¢˜', 'unknown')[:50],
                                    "product_url": item_data.get('å•†å“é“¾æ¥', 'unknown'),
                                    "retry_count": detail_retry_count,
                                    "max_retries": max_detail_retries,
                                    "current_proxy": proxy_address
                                }

                                error_msg = f"å•†å“è¯¦æƒ…ç½‘ç»œé”™è¯¯ (ç¬¬{detail_retry_count}æ¬¡): {error_str}"
                                await log_to_database(task_id, 'WARNING', error_msg, detail_error_info)
                                print(f"   -> {error_msg}")

                                # ç½‘ç»œé”™è¯¯æ—¶å°è¯•åˆ‡æ¢ä»£ç†
                                if detail_retry_count < max_detail_retries:
                                    await log_to_database(task_id, 'INFO', "å•†å“è¯¦æƒ…è·å–ç½‘ç»œé”™è¯¯ï¼Œå°è¯•åˆ‡æ¢ä»£ç†", {
                                        "action": "attempting_proxy_switch",
                                        "product_id": item_data.get('å•†å“ID', 'unknown')
                                    })
                                    print(f"   [ç½‘ç»œé”™è¯¯] å•†å“è¯¦æƒ…è·å–å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢ä»£ç†...")

                                    new_proxy = await handle_proxy_failure(task_id)
                                    if new_proxy and new_proxy != proxy_address:
                                        try:
                                            # å…³é—­å½“å‰ä¸Šä¸‹æ–‡
                                            await context.close()

                                            # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡å’Œé¡µé¢
                                            context = await create_browser_context(browser, new_proxy)
                                            page = await context.new_page()
                                            old_proxy = proxy_address
                                            proxy_address = new_proxy

                                            # è¯¦ç»†çš„ä»£ç†åˆ‡æ¢æˆåŠŸä¿¡æ¯
                                            switch_success_info = {
                                                "action": "product_detail_proxy_switch_success",
                                                "old_proxy": old_proxy,
                                                "new_proxy": proxy_address,
                                                "product_id": item_data.get('å•†å“ID', 'unknown'),
                                                "trigger_error": error_str
                                            }

                                            await log_to_database(task_id, 'INFO', f"å•†å“è¯¦æƒ…ä»£ç†åˆ‡æ¢æˆåŠŸ: {old_proxy} -> {proxy_address}", switch_success_info)
                                            print(f"   [ä»£ç†åˆ‡æ¢] å•†å“è¯¦æƒ…è·å–ä»£ç†åˆ‡æ¢æˆåŠŸ: {proxy_address}")

                                            # é‡ç½®é‡è¯•è®¡æ•°ï¼Œç»™æ–°ä»£ç†ä¸€ä¸ªæœºä¼š
                                            detail_retry_count = 0
                                            continue

                                        except Exception as proxy_error:
                                            switch_error_info = {
                                                "action": "product_detail_proxy_switch_failed",
                                                "old_proxy": proxy_address,
                                                "target_proxy": new_proxy,
                                                "error_message": str(proxy_error),
                                                "product_id": item_data.get('å•†å“ID', 'unknown')
                                            }

                                            await log_to_database(task_id, 'ERROR', f"å•†å“è¯¦æƒ…ä»£ç†åˆ‡æ¢å¤±è´¥: {str(proxy_error)}", switch_error_info)
                                            print(f"   [ä»£ç†åˆ‡æ¢] å•†å“è¯¦æƒ…è·å–ä»£ç†åˆ‡æ¢å¤±è´¥: {proxy_error}")

                                    # å¢åŠ å»¶è¿Ÿåé‡è¯•
                                    retry_delay = random.randint(5, 15)
                                    await log_to_database(task_id, 'INFO', f"å•†å“è¯¦æƒ…é‡è¯•å‰å»¶è¿Ÿ {retry_delay} ç§’", {
                                        "delay_seconds": retry_delay,
                                        "retry_reason": "product_detail_network_error",
                                        "product_id": item_data.get('å•†å“ID', 'unknown')
                                    })
                                    print(f"   [ç½‘ç»œé”™è¯¯] å•†å“è¯¦æƒ…è·å–é‡è¯•å‰å¢åŠ å»¶è¿Ÿ...")
                                    await asyncio.sleep(retry_delay)
                                else:
                                    # ç½‘ç»œé”™è¯¯é‡è¯•æ¬¡æ•°ç”¨å®Œ
                                    final_error_info = {
                                        "error_type": "product_detail_network_error_final",
                                        "product_id": item_data.get('å•†å“ID', 'unknown'),
                                        "product_title": item_data.get('å•†å“æ ‡é¢˜', 'unknown')[:50],
                                        "final_error": error_str,
                                        "total_retries": detail_retry_count
                                    }

                                    await log_to_database(task_id, 'ERROR', f"å•†å“è¯¦æƒ…ç½‘ç»œé”™è¯¯é‡è¯•å¤±è´¥: {item_data['å•†å“æ ‡é¢˜'][:30]}...", final_error_info)
                                    user_profile_data = {"è·å–çŠ¶æ€": f"ç½‘ç»œé”™è¯¯: {error_str}"}
                                    detail_fetch_success = True
                            else:
                                # éç½‘ç»œé”™è¯¯ï¼Œç«‹å³å¤±è´¥
                                error_msg = f"å¤„ç†å•†å“è¯¦æƒ…æ—¶å‘ç”Ÿé”™è¯¯: {error_str}"
                                await log_to_database(task_id, 'ERROR', error_msg)
                                print(f"   -> {error_msg}")
                                user_profile_data = {"è·å–çŠ¶æ€": f"å¤„ç†é”™è¯¯: {error_str}"}
                                detail_fetch_success = True

                    # æ„å»ºåŸºç¡€è®°å½•ï¼ˆæ— è®ºè¯¦æƒ…è·å–æ˜¯å¦æˆåŠŸï¼‰
                    final_record = {
                        "çˆ¬å–æ—¶é—´": datetime.now().isoformat(),
                        "æœç´¢å…³é”®å­—": keyword,
                        "ä»»åŠ¡åç§°": task_config.get('task_name', 'Untitled Task'),
                        "å•†å“ä¿¡æ¯": item_data,
                        "å–å®¶ä¿¡æ¯": user_profile_data,
                        "è¯¦æƒ…è·å–çŠ¶æ€": "æˆåŠŸ" if detail_fetch_success and user_profile_data.get("è·å–çŠ¶æ€") is None else user_profile_data.get("è·å–çŠ¶æ€", "å¤±è´¥")
                    }

                    # --- START: Real-time AI Analysis & Notification ---
                    print(f"   -> å¼€å§‹å¯¹å•†å“ #{item_data['å•†å“ID']} è¿›è¡Œå®æ—¶AIåˆ†æ...")
                    # 1. Download images
                    image_urls = item_data.get('å•†å“å›¾ç‰‡åˆ—è¡¨', [])
                    downloaded_image_paths = await download_all_images(item_data['å•†å“ID'], image_urls)

                    # 2. Get AI analysis
                    ai_analysis_result = None
                    if ai_prompt_text:
                        try:
                            ai_analysis_result = await get_ai_analysis(final_record, downloaded_image_paths, prompt_text=ai_prompt_text)
                            if ai_analysis_result:
                                final_record['ai_analysis'] = ai_analysis_result
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯çŠ¶æ€
                                if 'error' in ai_analysis_result:
                                    print(f"   -> AIåˆ†æå¤±è´¥: {ai_analysis_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                                else:
                                    print(f"   -> AIåˆ†æå®Œæˆã€‚æ¨èçŠ¶æ€: {ai_analysis_result.get('is_recommended')}")
                            else:
                                final_record['ai_analysis'] = {'error': 'AI analysis returned None after retries.', 'status': 'failed'}
                                print(f"   -> AIåˆ†æå¤±è´¥: é‡è¯•åä»è¿”å›ç©ºç»“æœ")
                        except Exception as e:
                            print(f"   -> AIåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                            final_record['ai_analysis'] = {'error': str(e), 'status': 'failed'}
                    else:
                        print("   -> ä»»åŠ¡æœªé…ç½®AI promptï¼Œè·³è¿‡åˆ†æã€‚")
                        final_record['ai_analysis'] = {'status': 'pending', 'reason': 'No AI prompt configured'}

                    # 3. Send notification if recommended (only for successful analysis)
                    if ai_analysis_result and ai_analysis_result.get('is_recommended') and 'error' not in ai_analysis_result:
                        print(f"   -> å•†å“è¢«AIæ¨èï¼Œå‡†å¤‡å‘é€é€šçŸ¥...")
                        
                        # å‘é€ntfyé€šçŸ¥
                        await send_ntfy_notification(item_data, ai_analysis_result.get("reason", "æ— "))
                        print(f"   -> é‚®ä»¶é€šçŸ¥æ¡ä»¶æ£€æŸ¥: email_enabled={email_enabled}, email_address={email_address}, smtp_configured={email_sender.is_configured()}")
                        # å‘é€é‚®ä»¶é€šçŸ¥
                        if email_enabled and email_address and email_sender.is_configured():
                            print(f"   -> å‡†å¤‡å‘é€é‚®ä»¶é€šçŸ¥åˆ°: {email_address}")
                            await log_to_database(task_id, 'INFO', f"å‡†å¤‡å‘é€é‚®ä»¶é€šçŸ¥: {final_record['å•†å“ä¿¡æ¯']['å•†å“æ ‡é¢˜'][:30]}...")
                            try:
                                email_success = await email_sender.send_product_notification(
                                    email_address,
                                    final_record,
                                    ai_analysis_result,
                                    task_name
                                )
                                
                                if email_success:
                                    print(f"   -> é‚®ä»¶é€šçŸ¥å‘é€æˆåŠŸ")
                                    await db.log_email_send(
                                        task_id, 
                                        processed_item_count,  # ä½¿ç”¨å•†å“åºå·ä½œä¸ºä¸´æ—¶ID
                                        email_address,
                                        f"ğŸš¨ é—²é±¼æ¨è | {item_data['å•†å“æ ‡é¢˜'][:30]}...",
                                        "success"
                                    )
                                else:
                                    print(f"   -> é‚®ä»¶é€šçŸ¥å‘é€å¤±è´¥")
                                    await db.log_email_send(
                                        task_id,
                                        processed_item_count,
                                        email_address,
                                        f"ğŸš¨ é—²é±¼æ¨è | {item_data['å•†å“æ ‡é¢˜'][:30]}...",
                                        "failed",
                                        "é‚®ä»¶å‘é€å¤±è´¥"
                                    )
                            except Exception as e:
                                print(f"   -> é‚®ä»¶é€šçŸ¥å‘é€å¼‚å¸¸: {e}")
                                await db.log_email_send(
                                    task_id,
                                    processed_item_count,
                                    email_address,
                                    f"ğŸš¨ é—²é±¼æ¨è | {item_data['å•†å“æ ‡é¢˜'][:30]}...",
                                    "error",
                                    str(e)
                                )
                        elif email_enabled and email_address:
                            print(f"   -> é‚®ä»¶é€šçŸ¥å·²å¯ç”¨ä½†SMTPé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡é‚®ä»¶å‘é€")
                    # --- END: Real-time AI Analysis & Notification ---

                    # 4. ä¿å­˜åŒ…å«AIç»“æœçš„å®Œæ•´è®°å½•åˆ°æ•°æ®åº“
                    await save_to_database(final_record, task_id)

                    processed_links.add(unique_key)
                    processed_item_count += 1
                    print(f"   -> å•†å“å¤„ç†æµç¨‹å®Œæ¯•ã€‚ç´¯è®¡å¤„ç† {processed_item_count} ä¸ªæ–°å•†å“ã€‚")

                    # --- ä¿®æ”¹: å¢åŠ å•ä¸ªå•†å“å¤„ç†åçš„ä¸»è¦å»¶è¿Ÿ ---
                    print("   [åçˆ¬] æ‰§è¡Œä¸€æ¬¡ä¸»è¦çš„éšæœºå»¶è¿Ÿä»¥æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆé—´éš”...")
                    await random_sleep(15, 30)

                    await detail_page.close()
                    await random_sleep(2, 4)

                # é¡µé¢é—´ä¼‘æ¯
                if not stop_scraping and page_num < max_pages:
                    await log_to_database(task_id, 'INFO', f"ç¬¬ {page_num} é¡µå¤„ç†å®Œæ¯•ï¼Œå‡†å¤‡ç¿»é¡µ")
                    print(f"--- ç¬¬ {page_num} é¡µå¤„ç†å®Œæ¯•ï¼Œå‡†å¤‡ç¿»é¡µã€‚æ‰§è¡Œä¸€æ¬¡é¡µé¢é—´çš„é•¿æ—¶ä¼‘æ¯... ---")
                    await random_sleep(25, 50)

        except PlaywrightTimeoutError as e:
            await log_to_database(task_id, 'ERROR', f"æ“ä½œè¶…æ—¶: {str(e)}")
            print(f"\næ“ä½œè¶…æ—¶é”™è¯¯: é¡µé¢å…ƒç´ æˆ–ç½‘ç»œå“åº”æœªåœ¨è§„å®šæ—¶é—´å†…å‡ºç°ã€‚\n{e}")
        except Exception as e:
            await log_to_database(task_id, 'ERROR', f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"\nçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        finally:
            # è®°å½•æœ€ç»ˆçš„ä»£ç†ä½¿ç”¨ç»Ÿè®¡
            await log_proxy_stats(task_id)
            await log_to_database(task_id, 'INFO', f"ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå…±å¤„ç† {processed_item_count} ä¸ªæ–°å•†å“")
            print("\nLOG: ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œæµè§ˆå™¨å°†åœ¨5ç§’åè‡ªåŠ¨å…³é—­...")
            await asyncio.sleep(5)
            if debug_limit:
                input("æŒ‰å›è½¦é”®å…³é—­æµè§ˆå™¨...")
            await browser.close()

    # è®°å½•ä»»åŠ¡æ‰§è¡Œå®Œæ¯•åˆ°æ•°æ®åº“
    await log_to_database(task_id, 'INFO', f"ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼Œå…±å¤„ç† {processed_item_count} ä¸ªæ–°å•†å“",
                        {"processed_count": processed_item_count})

    logger.info(f"ä»»åŠ¡å®Œæˆï¼Œå…±å¤„ç† {processed_item_count} ä¸ªæ–°å•†å“")
    return processed_item_count

def setup_task_logger(task_id: int, task_name: str):
    """ä¸ºæ¯ä¸ªä»»åŠ¡è®¾ç½®ç‹¬ç«‹çš„æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(f"task_{task_id}")
    logger.setLevel(logging.INFO)
    
    # é¿å…é‡å¤æ·»åŠ handler
    if logger.handlers:
        return logger
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs("logs", exist_ok=True)
    
    # åˆ›å»ºæ–‡ä»¶handler
    log_file = f"logs/{task_id}.log"
    file_handler = RotatingFileHandler(
        log_file, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8'
    )
    
    # åˆ›å»ºæ§åˆ¶å°handler
    console_handler = logging.StreamHandler()
    
    # è®¾ç½®æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

async def main():
    """
    ä¸»ç¨‹åºå…¥å£å‡½æ•°

    è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œä»æ•°æ®åº“åŠ è½½ä»»åŠ¡é…ç½®ï¼Œå¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬å–ä»»åŠ¡ã€‚
    æ”¯æŒè°ƒè¯•æ¨¡å¼ã€ä»»åŠ¡ç­›é€‰ç­‰åŠŸèƒ½ã€‚
    """
    parser = argparse.ArgumentParser(
        description="é—²é±¼å•†å“ç›‘æ§è„šæœ¬ï¼Œæ”¯æŒå¤šä»»åŠ¡é…ç½®å’Œå®æ—¶AIåˆ†æã€‚",
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è¿è¡Œæ•°æ®åº“ä¸­å®šä¹‰çš„æ‰€æœ‰å¯ç”¨ä»»åŠ¡
  python spider_v2.py

  # è°ƒè¯•æ¨¡å¼: è¿è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œä½†æ¯ä¸ªä»»åŠ¡åªå¤„ç†å‰3ä¸ªæ–°å‘ç°çš„å•†å“
  python spider_v2.py --debug-limit 3
""",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--debug-limit", type=int, default=0, help="è°ƒè¯•æ¨¡å¼ï¼šæ¯ä¸ªä»»åŠ¡ä»…å¤„ç†å‰ N ä¸ªæ–°å•†å“ï¼ˆ0 è¡¨ç¤ºæ— é™åˆ¶ï¼‰")
    args = parser.parse_args()

    # è®°å½•å½“å‰é…ç½®
    print(f"å½“å‰é…ç½® - è·³è¿‡å·²å­˜åœ¨å•†å“: {'æ˜¯' if SKIP_EXISTING_PRODUCTS else 'å¦'}")

    # åˆå§‹åŒ–æ•°æ®åº“
    await db.init_database()
    
    try:
        # ä»æ•°æ®åº“è·å–å¯ç”¨çš„ä»»åŠ¡é…ç½®
        tasks_config = await db.get_enabled_tasks()
    except Exception as e:
        sys.exit(f"é”™è¯¯: ä»æ•°æ®åº“è·å–ä»»åŠ¡é…ç½®å¤±è´¥: {e}")

    if not tasks_config:
        print("æ•°æ®åº“ä¸­æ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡ã€‚è¯·é€šè¿‡Webç•Œé¢æ·»åŠ ä»»åŠ¡ã€‚")
        return

    # è½¬æ¢æ•°æ®åº“æ ¼å¼ä¸ºåŸæœ‰çš„ä»»åŠ¡é…ç½®æ ¼å¼
    converted_tasks = []
    for task in tasks_config:
        converted_task = {
            'task_id': task['id'],
            'task_name': task['task_name'],
            'keyword': task['keyword'],
            'max_pages': task.get('max_pages', 3),
            'personal_only': task.get('personal_only', True),
            'ai_prompt_text': task.get('ai_prompt_text', ''),
            'email_enabled': task.get('email_enabled', False),
            'email_address': task.get('email_address', '')
        }
        
        # æ·»åŠ ä»·æ ¼èŒƒå›´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if task.get('min_price'):
            converted_task['min_price'] = task['min_price']
        if task.get('max_price'):
            converted_task['max_price'] = task['max_price']
            
        converted_tasks.append(converted_task)

    print("\n--- å¼€å§‹æ‰§è¡Œç›‘æ§ä»»åŠ¡ ---")
    if args.debug_limit > 0:
        print(f"** è°ƒè¯•æ¨¡å¼å·²æ¿€æ´»ï¼Œæ¯ä¸ªä»»åŠ¡æœ€å¤šå¤„ç† {args.debug_limit} ä¸ªæ–°å•†å“ **")
    print("--------------------")

    if not converted_tasks:
        print("æ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # ä¸ºæ¯ä¸ªå¯ç”¨çš„ä»»åŠ¡åˆ›å»ºä¸€ä¸ªå¼‚æ­¥æ‰§è¡Œåç¨‹
    coroutines = []
    for task_conf in converted_tasks:
        print(f"-> ä»»åŠ¡ '{task_conf['task_name']}' å·²åŠ å…¥æ‰§è¡Œé˜Ÿåˆ—ã€‚")
        coroutines.append(scrape_xianyu(task_config=task_conf, debug_limit=args.debug_limit))

    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    results = await asyncio.gather(*coroutines, return_exceptions=True)

    print("\n--- æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯• ---")
    for i, result in enumerate(results):
        task_name = converted_tasks[i]['task_name']
        task_id = converted_tasks[i]['task_id']
        if isinstance(result, Exception):
            error_message = f"ä»»åŠ¡ '{task_name}' å› å¼‚å¸¸è€Œç»ˆæ­¢: {result}"
            print(error_message)
            # è®°å½•å¼‚å¸¸åˆ°æ•°æ®åº“
            await log_to_database(task_id, 'ERROR', error_message)
        else:
            completion_message = f"ä»»åŠ¡ '{task_name}' æ­£å¸¸ç»“æŸï¼Œæœ¬æ¬¡è¿è¡Œå…±å¤„ç†äº† {result} ä¸ªæ–°å•†å“ã€‚"
            print(completion_message)
            # è®°å½•ä»»åŠ¡å®Œæˆåˆ°æ•°æ®åº“
            await log_to_database(task_id, 'INFO', completion_message,
                                {"processed_count": result, "status": "completed"})


@retry_on_failure(retries=3, delay=2)
async def get_proxy() -> Optional[str]:
    """
    ä»ä»£ç†APIè·å–ä»£ç†åœ°å€
    è¿”å›æ ¼å¼: "ip:port" æˆ– None (å¦‚æœè·å–å¤±è´¥)
    """
    if not PROXY_API_URL:
        print("   [ä»£ç†] æœªé…ç½®ä»£ç†API URL")
        return None
        
    try:
        print("   [ä»£ç†] æ­£åœ¨ä»APIè·å–ä»£ç†åœ°å€...")
        loop = asyncio.get_running_loop()
        
        # ä½¿ç”¨ run_in_executor æ‰§è¡ŒåŒæ­¥è¯·æ±‚
        response = await loop.run_in_executor(
            None,
            lambda: requests.get(PROXY_API_URL, timeout=10)
        )
        response.raise_for_status()
        
        data = response.json()
        
        # éªŒè¯å“åº”æ ¼å¼
        if data.get('code') != 200:
            print(f"   [ä»£ç†] APIè¿”å›é”™è¯¯: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
            return None
            
        proxy_list = data.get('data', {}).get('proxy_list', [])
        if not proxy_list:
            print("   [ä»£ç†] APIè¿”å›çš„ä»£ç†åˆ—è¡¨ä¸ºç©º")
            return None
            
        proxy_address = proxy_list[0]
        print(f"   [ä»£ç†] æˆåŠŸè·å–ä»£ç†: {proxy_address}")
        return proxy_address
        
    except requests.exceptions.RequestException as e:
        print(f"   [ä»£ç†] ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        raise
    except json.JSONDecodeError as e:
        print(f"   [ä»£ç†] JSONè§£æå¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"   [ä»£ç†] è·å–ä»£ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise

async def get_proxy_with_fallback(force_refresh: bool = False) -> Optional[str]:
    """
    è·å–ä»£ç†åœ°å€ï¼ˆå¸¦å›é€€æœºåˆ¶ï¼‰

    é€šè¿‡ä»£ç†ç®¡ç†å™¨è·å–å¯ç”¨çš„ä»£ç†åœ°å€ï¼Œæ”¯æŒå®šæ—¶è‡ªåŠ¨æ›´æ¢å’Œå¼ºåˆ¶åˆ·æ–°ã€‚
    å½“ä»£ç†è·å–å¤±è´¥æ—¶æä¾›ä¼˜é›…çš„é”™è¯¯å¤„ç†ã€‚

    Args:
        force_refresh (bool): æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ä»£ç†ï¼Œå¿½ç•¥æ—¶é—´é—´éš”é™åˆ¶

    Returns:
        Optional[str]: ä»£ç†åœ°å€å­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼šip:portï¼‰æˆ–Noneï¼ˆè·å–å¤±è´¥æ—¶ï¼‰
    """
    try:
        return await proxy_manager.get_fresh_proxy(force_refresh=force_refresh)
    except Exception as e:
        print(f"   [ä»£ç†ç®¡ç†] è·å–ä»£ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None

async def handle_proxy_failure(task_id: int) -> Optional[str]:
    """
    ä»£ç†å¤±æ•ˆå¤„ç†å‡½æ•°

    å½“æ£€æµ‹åˆ°å½“å‰ä»£ç†å¤±æ•ˆæ—¶ï¼Œç«‹å³å°è¯•è·å–æ–°çš„ä»£ç†åœ°å€ã€‚
    åŒ…å«å®Œæ•´çš„æ—¥å¿—è®°å½•å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚

    Args:
        task_id (int): ä»»åŠ¡IDï¼Œç”¨äºå…³è”æ—¥å¿—è®°å½•å’Œé”™è¯¯è¿½è¸ª

    Returns:
        Optional[str]: æ–°çš„ä»£ç†åœ°å€æˆ–Noneï¼ˆå¦‚æœæ— æ³•è·å–æ–°ä»£ç†ï¼‰
    """
    await log_to_database(task_id, 'WARNING', "æ£€æµ‹åˆ°ä»£ç†å¤±æ•ˆï¼Œç«‹å³å°è¯•æ›´æ¢ä»£ç†")
    print("   [ä»£ç†ç®¡ç†] æ£€æµ‹åˆ°ä»£ç†å¤±æ•ˆï¼Œç«‹å³å°è¯•æ›´æ¢ä»£ç†...")
    
    # å¼ºåˆ¶åˆ·æ–°ä»£ç†
    new_proxy = await get_proxy_with_fallback(force_refresh=True)
    
    if new_proxy:
        await log_to_database(task_id, 'INFO', f"æˆåŠŸæ›´æ¢ä¸ºæ–°ä»£ç†: {new_proxy}")
        print(f"   [ä»£ç†ç®¡ç†] æˆåŠŸæ›´æ¢ä¸ºæ–°ä»£ç†: {new_proxy}")
        # è®°å½•ä»£ç†ä½¿ç”¨
        proxy_manager.record_usage()
    else:
        await log_to_database(task_id, 'ERROR', "æ— æ³•è·å–æ–°ä»£ç†ï¼Œå°†ç»§ç»­æ— ä»£ç†æ¨¡å¼")
        print("   [ä»£ç†ç®¡ç†] æ— æ³•è·å–æ–°ä»£ç†ï¼Œå°†ç»§ç»­æ— ä»£ç†æ¨¡å¼")
        
    return new_proxy

async def log_proxy_stats(task_id: int):
    """è®°å½•ä»£ç†ä½¿ç”¨ç»Ÿè®¡åˆ°æ•°æ®åº“"""
    stats = proxy_manager.get_proxy_stats()
    
    if stats["status"] == "active":
        await log_to_database(task_id, 'INFO', 
            f"ä»£ç†ä½¿ç”¨ç»Ÿè®¡ - åœ°å€: {stats['address']}, "
            f"ä½¿ç”¨æ—¶é•¿: {stats['usage_time']:.1f}ç§’, "
            f"ä½¿ç”¨æ¬¡æ•°: {stats['usage_count']}, "
            f"å‰©ä½™æ—¶é—´: {stats['remaining_time']:.1f}ç§’"
        )

if __name__ == "__main__":
    asyncio.run(main())
